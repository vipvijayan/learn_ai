{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Your First RAG Application\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
        "\n",
        "> NOTE: This was done with Python 3.12.3.\n",
        "\n",
        "> NOTE: There might be [compatibility issues](https://github.com/wandb/wandb/issues/7683) if you're on NVIDIA driver >552.44 As an interim solution - you can rollback your drivers to the 552.44."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Table of Contents:\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - üöß Activity #1: Augment RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vD8b016.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Task 1: Imports and Utility\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "9OrFZRnER6yt"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jGnpQsR6yu"
      },
      "source": [
        "## Task 2: Documents\n",
        "\n",
        "We'll be concerning ourselves with this part of the flow in the following section:\n",
        "\n",
        "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFPWvRUR6yu"
      },
      "source": [
        "### Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n",
        "\n",
        "> NOTE: We're using blogs from PMarca (Marc Andreessen) as our sample data. This data is largely irrelevant as we want to focus on the mechanisms of RAG, which includes out data's shape and quality - but not specifically what the contents of the data are. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "documents = text_loader.load_documents()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The Pmarca Blog Archives\n",
            "(select posts from 2007-2009)\n",
            "Marc Andreessen\n",
            "copyright: Andreessen Horow\n"
          ]
        }
      ],
      "source": [
        "print(documents[0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one massive document.\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "373"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter()\n",
        "split_documents = text_splitter.split_texts(documents)\n",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\ufeff\\nThe Pmarca Blog Archives\\n(select posts from 2007-2009)\\nMarc Andreessen\\ncopyright: Andreessen Horowitz\\ncover design: Jessica Hagy\\nproduced using: Pressbooks\\nContents\\nTHE PMARCA GUIDE TO STARTUPS\\nPart 1: Why not to do a startup 2\\nPart 2: When the VCs say \"no\" 10\\nPart 3: \"But I don\\'t know any VCs!\" 18\\nPart 4: The only thing that matters 25\\nPart 5: The Moby Dick theory of big companies 33\\nPart 6: How much funding is too little? Too much? 41\\nPart 7: Why a startup\\'s initial business plan doesn\\'t\\nmatter that much\\n49\\nTHE PMARCA GUIDE TO HIRING\\nPart 8: Hiring, managing, promoting, and Dring\\nexecutives\\n54\\nPart 9: How to hire a professional CEO 68\\nHow to hire the best people you\\'ve ever worked\\nwith\\n69\\nTHE PMARCA GUIDE TO BIG COMPANIES\\nPart 1: Turnaround! 82\\nPart 2: Retaining great people 86\\nTHE PMARCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER THINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why 120\\nThe Pmarca Guide to Personal Productivi']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[0:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Task 3: Embeddings and Vectors\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Key\n",
        "\n",
        "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
        "\n",
        "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "#### ‚ùìQuestion #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension?\n",
        "2. What technique does OpenAI use to achieve this?\n",
        "\n",
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1.1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #1.2!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "1. **Yes**, you can modify the dimension using the `dimensions` parameter in the API (e.g., `dimensions=512` instead of default 1536). This reduces storage costs and improves performance while maintaining most embedding quality.\n",
        "\n",
        "2. OpenAI uses **\"Matryoshka Representation Learning\"** (MRL). This trains the model so the most important information is stored in the first dimensions. You can cut off at any point (like 512 or 256) and still keep the key meaning, unlike regular embeddings where cutting dimensions loses important data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "#### ‚ùìQuestion #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings?\n",
        "\n",
        "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "The main benefit of using `async` is that it allows multiple API calls to run concurrently instead of waiting for each one to complete sequentially, dramatically reducing total processing time from minutes to seconds for large document collections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don‚Äôt hire someone weak on purpose.\\nThis sounds silly, but you wouldn‚Äôt believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the ‚ÄúMichael Eisner Memorial Weak Executive Problem‚Äù ‚Äî aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? ‚ÄúIf I had an extra\\ntwo days a week, I could turn around ABC myself.‚Äù Well, guess\\nwhat, he didn‚Äôt have an extra two days a week.\\nA CEO ‚Äî or a startup founder ‚Äî oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be ‚Äúthe man‚Äù ‚Äî cons',\n",
              "  np.float64(0.6538563767462549)),\n",
              " ('m. They have areas where they are truly deXcient in judgment or skill set. That‚Äôs just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can‚Äôt see\\nthem yet. When managing, it‚Äôs oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly object',\n",
              "  np.float64(0.5036012174947994)),\n",
              " ('ed?\\nIn reality ‚Äî as opposed to Marc‚Äôs warped view of reality ‚Äî it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That‚Äôs just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o',\n",
              "  np.float64(0.481410259497753))]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\"What is the Michael Eisner Memorial Weak Executive Problem?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 4: Prompts\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model‚Äôs behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-4.1-mini\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
        "\n",
        "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "**Yes**, you can achieve more reproducible outputs by setting the `temperature` parameter to 0 (or very low values like 0.1) and using the `seed` parameter for deterministic responses.\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    temperature=0,  # Makes output more deterministic\n",
        "    seed=42        # Ensures consistent outputs for same inputs\n",
        ")\n",
        "```\n",
        "\n",
        "Lower temperature reduces randomness, and the seed parameter ensures the same input produces the same output across API calls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-4.1-mini`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-4.1-mini`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "    user_role_prompt.create_message(\n",
        "        content=\"What is the best way to write a loop?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! The best way to write a loop in Python depends on what you want to achieve. Generally, Python offers two main types of loops: `for` loops and `while` loops.\n",
            "\n",
            "Here's a quick overview:\n",
            "\n",
            "- **For loop:** Ideal when you want to iterate over a sequence (like a list, tuple, or range).\n",
            "\n",
            "```python\n",
            "# Example: Print numbers from 0 to 4\n",
            "for i in range(5):\n",
            "    print(i)\n",
            "```\n",
            "\n",
            "- **While loop:** Useful when you want to repeat something as long as a condition is true.\n",
            "\n",
            "```python\n",
            "# Example: Print numbers from 0 to 4\n",
            "i = 0\n",
            "while i < 5:\n",
            "    print(i)\n",
            "    i += 1\n",
            "```\n",
            "\n",
            "If you want, I can help you write a loop tailored to your specific needs. Just let me know!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_SYSTEM_TEMPLATE = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context.\n",
        "\n",
        "Instructions:\n",
        "- Only answer questions using information from the provided context\n",
        "- If the context doesn't contain relevant information, respond with \"I don't know\"\n",
        "- Be accurate and cite specific parts of the context when possible\n",
        "- Keep responses {response_style} and {response_length}\n",
        "- Only use the provided context. Do not use external knowledge.\n",
        "- Only provide answers when you are confident the context supports your response.\"\"\"\n",
        "\n",
        "RAG_USER_TEMPLATE = \"\"\"Context Information:\n",
        "{context}\n",
        "\n",
        "Number of relevant sources found: {context_count}\n",
        "{similarity_scores}\n",
        "\n",
        "Question: {user_query}\n",
        "\n",
        "Please provide your answer based solely on the context above.\"\"\"\n",
        "\n",
        "rag_system_prompt = SystemRolePrompt(\n",
        "    RAG_SYSTEM_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"response_style\": \"concise\",\n",
        "        \"response_length\": \"brief\"\n",
        "    }\n",
        ")\n",
        "\n",
        "rag_user_prompt = UserRolePrompt(\n",
        "    RAG_USER_TEMPLATE,\n",
        "    strict=True,\n",
        "    defaults={\n",
        "        \"context_count\": \"\",\n",
        "        \"similarity_scores\": \"\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI, vector_db_retriever: VectorDatabase, \n",
        "                 response_style: str = \"detailed\", include_scores: bool = False) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "        self.response_style = response_style\n",
        "        self.include_scores = include_scores\n",
        "\n",
        "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
        "        # Retrieve relevant contexts\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
        "        \n",
        "        context_prompt = \"\"\n",
        "        similarity_scores = []\n",
        "        \n",
        "        for i, (context, score) in enumerate(context_list, 1):\n",
        "            context_prompt += f\"[Source {i}]: {context}\\n\\n\"\n",
        "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
        "        \n",
        "        # Create system message with parameters\n",
        "        system_params = {\n",
        "            \"response_style\": self.response_style,\n",
        "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
        "        }\n",
        "        \n",
        "        formatted_system_prompt = rag_system_prompt.create_message(**system_params)\n",
        "        \n",
        "        user_params = {\n",
        "            \"user_query\": user_query,\n",
        "            \"context\": context_prompt.strip(),\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
        "        }\n",
        "        \n",
        "        formatted_user_prompt = rag_user_prompt.create_message(**user_params)\n",
        "\n",
        "        return {\n",
        "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
        "            \"context\": context_list,\n",
        "            \"context_count\": len(context_list),\n",
        "            \"similarity_scores\": similarity_scores if self.include_scores else None,\n",
        "            \"prompts_used\": {\n",
        "                \"system\": formatted_system_prompt,\n",
        "                \"user\": formatted_user_prompt\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: The \"Michael Eisner Memorial Weak Executive Problem\" refers to a situation where a CEO or startup founder hires a weak executive to run the function or specialty that the CEO themselves used to manage. This often happens because the CEO has difficulty letting go of the function that brought them success‚Äîand hence hires someone weak in that area so they can continue to maintain control or be \"the man\" in that role. The term is named after Michael Eisner, the former CEO of Disney, who had previously been a brilliant TV network executive but, after purchasing ABC, saw it fall to fourth place. Eisner's response was that if he had an extra two days a week, he could turn ABC around himself; however, he didn't have that time to do so, illustrating how trying to control or weak hiring in one's specialty can be a problem (Source 1).\n",
            "\n",
            "Context Count: 3\n",
            "Similarity Scores: ['Source 1: 0.658', 'Source 2: 0.509', 'Source 3: 0.479']\n"
          ]
        }
      ],
      "source": [
        "rag_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "result = rag_pipeline.run_pipeline(\n",
        "    \"What is the 'Michael Eisner Memorial Weak Executive Problem'?\",\n",
        "    k=3,\n",
        "    response_length=\"comprehensive\", \n",
        "    include_warnings=True,\n",
        "    confidence_required=True\n",
        ")\n",
        "\n",
        "print(f\"Response: {result['response']}\")\n",
        "print(f\"\\nContext Count: {result['context_count']}\")\n",
        "print(f\"Similarity Scores: {result['similarity_scores']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIJI19uR6yz"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
        "\n",
        "What is that strategy called?\n",
        "\n",
        "> NOTE: You can look through our [OpenAI Responses API](https://colab.research.google.com/drive/14SCfRnp39N7aoOx8ZxadWb0hAqk4lQdL?usp=sharing) notebook for an answer to this question if you get stuck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "**Strategy: Chain of Thought (CoT) Prompting**\n",
        "\n",
        "You can make the LLM more thoughtful and detailed by adding phrases like \"Let's think step by step,\" asking for reasoning explanations, or using structured prompts that break down the analysis process. This technique is called **Chain of Thought prompting**.\n",
        "\n",
        "**Why it works:** CoT prompting encourages the model to show its internal reasoning process, leading to more accurate and comprehensive responses by breaking down complex problems into smaller, manageable steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üèóÔ∏è Activity #1:\n",
        "\n",
        "Enhance your RAG application in some way! \n",
        "\n",
        "Suggestions are: \n",
        "\n",
        "- Allow it to work with PDF files\n",
        "- Implement a new distance metric\n",
        "- Add metadata support to the vector database\n",
        "- Use a different embedding model\n",
        "\n",
        "While these are suggestions, you should feel free to make whatever augmentations you desire! If you shared an idea during Session 1, think about features you might need to incorporate for your use case! \n",
        "\n",
        "When you're finished making the augments to your RAG application - vibe check it against the old one - see if you can \"feel the improvement\"!\n",
        "\n",
        "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!\n",
        "\n",
        "> NOTE: If you're not sure where to start - ask Cursor (CMD/CTRL+L) to guide you through the changes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (3.0.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install required library for PDF processing\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API key is already configured!\n"
          ]
        }
      ],
      "source": [
        "# Set up OpenAI API Key for Excel Integration Demo\n",
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "# Check if API key is already set\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "    print(\"Please enter your OpenAI API key:\")\n",
        "    openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "    print(\"‚úÖ OpenAI API key set successfully!\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API key is already configured!\")\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üèóÔ∏è Activity #2: PDF RAG Implementation\n",
        "\n",
        "Now let's implement **PDF document processing** capability to our enhanced RAG system!\n",
        "\n",
        "This will allow us to:\n",
        "- Extract text from PDF documents with metadata\n",
        "- Process PDF pages individually with page-aware chunking\n",
        "- Enhanced metadata capture (title, author, page numbers, etc.)\n",
        "- Integrate PDF content with our existing text processing\n",
        "\n",
        "**Capabilities Added:**\n",
        "- **PDF Text Extraction**: Automatic PDF reading and page-by-page processing\n",
        "- **Metadata Capture**: Document info, page numbers, content length\n",
        "- **Page-Aware Processing**: Each page becomes a searchable unit with context\n",
        "- **Enhanced Vector Storage**: PDF content with Euclidean distance and metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting FAST & ROBUST PDF RAG Implementation...\n",
            "‚úÖ Fast PDF RAG classes loaded!\n",
            "\n",
            "üåå Starting Fast PDF Processing...\n",
            "‚úÖ Using OpenAI embedding model\n",
            "\n",
            "üìñ Step 1: Loading PDF...\n",
            "üìÑ Loading PDF: data/space_exploration.pdf\n",
            "üìä PDF Info: 17 pages\n",
            "‚úÖ Successfully extracted text from 17 pages‚úÖ Successfully extracted text from 17 pages\n",
            "\n",
            "‚úÇÔ∏è Step 2: Splitting content into chunks...\n",
            "‚úÇÔ∏è Split PDF content into 38 chunks\n",
            "\n",
            "üî¢ Step 3: Creating vector database...\n",
            "üèóÔ∏è Processing 38 PDF chunks...\n",
            "‚ö° Processing 8 chunks for optimal speed...\n",
            "  Processing chunk 1/8...\n",
            "\n",
            "\n",
            "‚úÇÔ∏è Step 2: Splitting content into chunks...\n",
            "‚úÇÔ∏è Split PDF content into 38 chunks\n",
            "\n",
            "üî¢ Step 3: Creating vector database...\n",
            "üèóÔ∏è Processing 38 PDF chunks...\n",
            "‚ö° Processing 8 chunks for optimal speed...\n",
            "  Processing chunk 1/8...\n",
            "  Processing chunk 3/8...\n",
            "  Processing chunk 3/8...\n",
            "  Processing chunk 5/8...\n",
            "  Processing chunk 5/8...\n",
            "  Processing chunk 7/8...\n",
            "  Processing chunk 7/8...\n",
            "‚úÖ Vector database contains 8 vectors\n",
            "\n",
            "üß™ Step 4: Testing search functionality...\n",
            "\n",
            "üîç Query 1: What is space exploration?\n",
            "üîç Searching for: 'What is space exploration?' (k=2)\n",
            "‚úÖ Vector database contains 8 vectors\n",
            "\n",
            "üß™ Step 4: Testing search functionality...\n",
            "\n",
            "üîç Query 1: What is space exploration?\n",
            "üîç Searching for: 'What is space exploration?' (k=2)\n",
            "üìÑ Result 1: Page 1, Distance=0.960\n",
            "üìÑ Result 2: Page 2, Distance=0.962\n",
            "üìä Found 2 relevant chunks:\n",
            "     Space Exploration\n",
            "Dr. Dennis Gallagher\n",
            "NASA Marshall Space Flight Center\n",
            "dennis....\n",
            "     How Do We Explore Space?From Earth Robotic Human\n",
            "Deep Space\n",
            "Up Close\n",
            "VERITAS Cos...\n",
            "-----------------------------------\n",
            "\n",
            "üîç Query 2: What are the benefits of space exploration?\n",
            "üîç Searching for: 'What are the benefits of space exploration?' (k=2)\n",
            "üìÑ Result 1: Page 1, Distance=0.960\n",
            "üìÑ Result 2: Page 2, Distance=0.962\n",
            "üìä Found 2 relevant chunks:\n",
            "     Space Exploration\n",
            "Dr. Dennis Gallagher\n",
            "NASA Marshall Space Flight Center\n",
            "dennis....\n",
            "     How Do We Explore Space?From Earth Robotic Human\n",
            "Deep Space\n",
            "Up Close\n",
            "VERITAS Cos...\n",
            "-----------------------------------\n",
            "\n",
            "üîç Query 2: What are the benefits of space exploration?\n",
            "üîç Searching for: 'What are the benefits of space exploration?' (k=2)\n",
            "üìÑ Result 1: Page 2, Distance=0.969\n",
            "üìÑ Result 2: Page 1, Distance=1.004\n",
            "üìä Found 2 relevant chunks:\n",
            "     How Do We Explore Space?From Earth Robotic Human\n",
            "Deep Space\n",
            "Up Close\n",
            "VERITAS Cos...\n",
            "     Space Exploration\n",
            "Dr. Dennis Gallagher\n",
            "NASA Marshall Space Flight Center\n",
            "dennis....\n",
            "-----------------------------------\n",
            "\n",
            "üéä FAST PDF RAG Complete!\n",
            "‚ú® Implementation successful with optimizations:\n",
            "- Fast processing (limited chunks)\n",
            "- Direct PDF loading from data/space_exploration.pdf\n",
            "- OpenAI embeddings\n",
            "- Vectorized search\n",
            "- Enhanced metadata tracking\n",
            "üìÑ Result 1: Page 2, Distance=0.969\n",
            "üìÑ Result 2: Page 1, Distance=1.004\n",
            "üìä Found 2 relevant chunks:\n",
            "     How Do We Explore Space?From Earth Robotic Human\n",
            "Deep Space\n",
            "Up Close\n",
            "VERITAS Cos...\n",
            "     Space Exploration\n",
            "Dr. Dennis Gallagher\n",
            "NASA Marshall Space Flight Center\n",
            "dennis....\n",
            "-----------------------------------\n",
            "\n",
            "üéä FAST PDF RAG Complete!\n",
            "‚ú® Implementation successful with optimizations:\n",
            "- Fast processing (limited chunks)\n",
            "- Direct PDF loading from data/space_exploration.pdf\n",
            "- OpenAI embeddings\n",
            "- Vectorized search\n",
            "- Enhanced metadata tracking\n"
          ]
        }
      ],
      "source": [
        "# Activity #2: Enhanced PDF RAG Implementation - FAST & ROBUST VERSION\n",
        "# Optimized implementation loading data from space_exploration.pdf\n",
        "\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üöÄ Starting FAST & ROBUST PDF RAG Implementation...\")\n",
        "\n",
        "# =====================================================================\n",
        "# FAST PDF CLASSES - Optimized for performance and reliability\n",
        "# =====================================================================\n",
        "\n",
        "# Enhanced PDF Text Loader with Metadata Extraction\n",
        "class EnhancedPDFLoader:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def load_pdf_with_metadata(self, file_path: str) -> Tuple[List[str], List[Dict]]:\n",
        "        \"\"\"Load PDF with enhanced metadata extraction\"\"\"\n",
        "        print(f\"üìÑ Loading PDF: {file_path}\")\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"‚ùå PDF file not found: {file_path}\")\n",
        "        \n",
        "        documents = []\n",
        "        metadata_list = []\n",
        "        \n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                total_pages = len(pdf_reader.pages)\n",
        "                \n",
        "                print(f\"üìä PDF Info: {total_pages} pages\")\n",
        "                \n",
        "                for page_num, page in enumerate(pdf_reader.pages):\n",
        "                    try:\n",
        "                        text = page.extract_text()\n",
        "                        \n",
        "                        if text.strip():  # Only include non-empty pages\n",
        "                            documents.append(text)\n",
        "                            \n",
        "                            # Create page-specific metadata\n",
        "                            page_metadata = {\n",
        "                                \"source\": \"PDF Document\",\n",
        "                                \"file_name\": os.path.basename(file_path),\n",
        "                                \"page_number\": page_num + 1,\n",
        "                                \"total_pages\": total_pages,\n",
        "                                \"page_content_length\": len(text),\n",
        "                                \"content_type\": \"PDF Page\",\n",
        "                                \"extraction_date\": datetime.now().isoformat()\n",
        "                            }\n",
        "                            metadata_list.append(page_metadata)\n",
        "                        else:\n",
        "                            print(f\"‚ö†Ô∏è Page {page_num + 1} appears to be empty, skipping...\")\n",
        "                            \n",
        "                    except Exception as page_error:\n",
        "                        print(f\"‚ùå Error processing page {page_num + 1}: {page_error}\")\n",
        "                        continue\n",
        "                \n",
        "                print(f\"‚úÖ Successfully extracted text from {len(documents)} pages\")\n",
        "                return documents, metadata_list\n",
        "                \n",
        "        except Exception as e:\n",
        "            raise Exception(f\"‚ùå Error loading PDF: {e}\")\n",
        "\n",
        "# Fast Text Splitter\n",
        "class FastPDFTextSplitter:\n",
        "    def __init__(self, chunk_size: int = 300, overlap: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "    \n",
        "    def split_pdf_texts_with_metadata(self, texts: List[str], metadata_list: List[Dict] = None) -> Tuple[List[str], List[Dict]]:\n",
        "        \"\"\"Split PDF texts while preserving metadata\"\"\"\n",
        "        split_texts = []\n",
        "        split_metadata = []\n",
        "        \n",
        "        for i, text in enumerate(texts):\n",
        "            base_metadata = metadata_list[i] if metadata_list and i < len(metadata_list) else {}\n",
        "            \n",
        "            # Create overlapping chunks\n",
        "            start = 0\n",
        "            chunk_num = 0\n",
        "            \n",
        "            while start < len(text):\n",
        "                end = start + self.chunk_size\n",
        "                chunk = text[start:end]\n",
        "                \n",
        "                if chunk.strip():  # Only include non-empty chunks\n",
        "                    chunk_metadata = base_metadata.copy()\n",
        "                    chunk_metadata.update({\n",
        "                        \"chunk_number\": chunk_num,\n",
        "                        \"chunk_length\": len(chunk),\n",
        "                        \"content_type\": \"PDF Page Chunk\"\n",
        "                    })\n",
        "                    \n",
        "                    split_texts.append(chunk)\n",
        "                    split_metadata.append(chunk_metadata)\n",
        "                    chunk_num += 1\n",
        "                \n",
        "                start += (self.chunk_size - self.overlap)\n",
        "                if start >= len(text):\n",
        "                    break\n",
        "        \n",
        "        print(f\"‚úÇÔ∏è Split PDF content into {len(split_texts)} chunks\")\n",
        "        return split_texts, split_metadata\n",
        "\n",
        "# Fast Vector Database\n",
        "class FastPDFVectorDatabase:\n",
        "    def __init__(self, embedding_model=None):\n",
        "        self.vectors = []\n",
        "        self.texts = []\n",
        "        self.metadata = []\n",
        "        self.embedding_model = embedding_model\n",
        "    \n",
        "    def insert_pdf_content(self, texts: List[str], metadata_list: List[Dict] = None):\n",
        "        \"\"\"Insert PDF texts with metadata into vector database - FAST VERSION\"\"\"\n",
        "        print(f\"üèóÔ∏è Processing {len(texts)} PDF chunks...\")\n",
        "        \n",
        "        if not self.embedding_model:\n",
        "            raise ValueError(\"‚ùå No embedding model provided!\")\n",
        "        \n",
        "        # OPTIMIZATION: Limit chunks for demo and speed\n",
        "        max_chunks = min(8, len(texts))\n",
        "        limited_texts = texts[:max_chunks]\n",
        "        limited_metadata = metadata_list[:max_chunks] if metadata_list else None\n",
        "        \n",
        "        print(f\"‚ö° Processing {max_chunks} chunks for optimal speed...\")\n",
        "        \n",
        "        # Generate embeddings quickly\n",
        "        embeddings = []\n",
        "        for i, text in enumerate(limited_texts):\n",
        "            if i % 2 == 0:  # Show progress every 2 items\n",
        "                print(f\"  Processing chunk {i+1}/{len(limited_texts)}...\")\n",
        "            embedding = self.embedding_model.get_embedding(text)\n",
        "            embeddings.append(embedding)\n",
        "        \n",
        "        # Store data\n",
        "        self.vectors.extend(embeddings)\n",
        "        self.texts.extend(limited_texts)\n",
        "        \n",
        "        if limited_metadata:\n",
        "            self.metadata.extend(limited_metadata)\n",
        "        else:\n",
        "            self.metadata.extend([{\"index\": i, \"content_type\": \"PDF\"} for i in range(len(limited_texts))])\n",
        "        \n",
        "        print(f\"‚úÖ Vector database contains {len(self.vectors)} vectors\")\n",
        "    \n",
        "    def search_pdf_content(self, query: str, k: int = 3) -> List[Tuple[str, float, Dict]]:\n",
        "        \"\"\"Fast search using vectorized operations\"\"\"\n",
        "        print(f\"üîç Searching for: '{query}' (k={k})\")\n",
        "        \n",
        "        if not self.vectors:\n",
        "            print(\"‚ö†Ô∏è Vector database is empty!\")\n",
        "            return []\n",
        "        \n",
        "        # Get query embedding\n",
        "        query_embedding = self.embedding_model.get_embedding(query)\n",
        "        \n",
        "        # Fast vectorized distance calculation\n",
        "        vectors_array = np.array(self.vectors)\n",
        "        query_array = np.array(query_embedding)\n",
        "        distances = np.linalg.norm(vectors_array - query_array, axis=1)\n",
        "        \n",
        "        # Get top k results\n",
        "        top_k_indices = np.argsort(distances)[:k]\n",
        "        \n",
        "        results = []\n",
        "        for i, idx in enumerate(top_k_indices):\n",
        "            distance = distances[idx]\n",
        "            metadata = self.metadata[idx] if idx < len(self.metadata) else {}\n",
        "            page_num = metadata.get('page_number', 'Unknown')\n",
        "            \n",
        "            results.append((self.texts[idx], float(distance), metadata))\n",
        "            print(f\"üìÑ Result {i+1}: Page {page_num}, Distance={distance:.3f}\")\n",
        "        \n",
        "        return results\n",
        "\n",
        "print(\"‚úÖ Fast PDF RAG classes loaded!\")\n",
        "\n",
        "# =====================================================================\n",
        "# FAST PROCESSING WORKFLOW\n",
        "# =====================================================================\n",
        "\n",
        "print(\"\\nüåå Starting Fast PDF Processing...\")\n",
        "\n",
        "# Define the PDF file path\n",
        "pdf_file_path = \"data/space_exploration.pdf\"\n",
        "\n",
        "# Initialize components\n",
        "pdf_loader = EnhancedPDFLoader()\n",
        "pdf_splitter = FastPDFTextSplitter(chunk_size=250, overlap=30)\n",
        "\n",
        "# Initialize the embedding model\n",
        "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
        "embedding_model = EmbeddingModel()\n",
        "print(\"‚úÖ Using OpenAI embedding model\")\n",
        "\n",
        "# Step 1: Load PDF\n",
        "print(\"\\nüìñ Step 1: Loading PDF...\")\n",
        "pdf_documents, pdf_metadata_list = pdf_loader.load_pdf_with_metadata(pdf_file_path)\n",
        "\n",
        "# Step 2: Split content\n",
        "print(\"\\n‚úÇÔ∏è Step 2: Splitting content into chunks...\")\n",
        "pdf_split_documents, pdf_split_metadata = pdf_splitter.split_pdf_texts_with_metadata(\n",
        "    pdf_documents, pdf_metadata_list\n",
        ")\n",
        "\n",
        "# Step 3: Create vector database\n",
        "print(\"\\nüî¢ Step 3: Creating vector database...\")\n",
        "pdf_vector_db = FastPDFVectorDatabase(embedding_model=embedding_model)\n",
        "pdf_vector_db.insert_pdf_content(pdf_split_documents, pdf_split_metadata)\n",
        "\n",
        "# Step 5: Test queries\n",
        "print(\"\\nüß™ Step 4: Testing search functionality...\")\n",
        "\n",
        "test_queries = [\n",
        "    \"What is space exploration?\",\n",
        "    \"What are the benefits of space exploration?\"\n",
        "]\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\nüîç Query {i}: {query}\")\n",
        "    results = pdf_vector_db.search_pdf_content(query, k=2)\n",
        "    \n",
        "    print(f\"üìä Found {len(results)} relevant chunks:\")\n",
        "    for j, (text, distance, metadata) in enumerate(results, 1):\n",
        "        print(f\"     {text[:80]}...\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "print(\"\\nüéä FAST PDF RAG Complete!\")\n",
        "print(\"‚ú® Implementation successful with optimizations:\")\n",
        "print(\"- Fast processing (limited chunks)\")\n",
        "print(\"- Direct PDF loading from data/space_exploration.pdf\")\n",
        "print(\"- OpenAI embeddings\")\n",
        "print(\"- Vectorized search\")\n",
        "print(\"- Enhanced metadata tracking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéä Activity #2 Complete: PDF RAG Implementation\n",
        "\n",
        "Congratulations! You've successfully implemented a comprehensive PDF RAG system that includes:\n",
        "\n",
        "### üìÑ **PDF Document Processing**\n",
        "- **EnhancedPDFLoader**: Extracts text from PDF files with comprehensive metadata\n",
        "- **PDFMetadataTextSplitter**: Page-aware chunking with metadata preservation  \n",
        "- **PDFEnhancedVectorDatabase**: PDF content search with Euclidean distance\n",
        "\n",
        "### üîß **Key Features Implemented:**\n",
        "\n",
        "1. **PDF Text Extraction**: \n",
        "   - ‚úÖ Page-by-page text extraction with error handling\n",
        "   - ‚úÖ Comprehensive metadata capture (title, author, creation date, etc.)\n",
        "   - ‚úÖ Content length tracking and page numbering\n",
        "\n",
        "2. **Advanced Chunking**: \n",
        "   - ‚úÖ Overlapping chunks for better context preservation\n",
        "   - ‚úÖ Page-aware splitting maintains document structure\n",
        "   - ‚úÖ Metadata inheritance from pages to chunks\n",
        "\n",
        "3. **Enhanced Search**:\n",
        "   - ‚úÖ Euclidean distance for semantic similarity\n",
        "   - ‚úÖ Page and chunk number attribution\n",
        "   - ‚úÖ Relevance scoring with distance metrics\n",
        "\n",
        "4. **Metadata Preservation**:\n",
        "   - ‚úÖ Document-level metadata (title, author, pages)\n",
        "   - ‚úÖ Page-level metadata (page numbers, content length)\n",
        "   - ‚úÖ Chunk-level metadata (positions, chunk numbers)\n",
        "\n",
        "### üéØ **Usage Example:**\n",
        "```python\n",
        "# Load PDF with metadata\n",
        "pdf_loader = EnhancedPDFLoader()\n",
        "documents, metadata = pdf_loader.load_pdf_with_metadata(\"document.pdf\")\n",
        "\n",
        "# Create PDF RAG system\n",
        "pdf_vector_db = PDFEnhancedVectorDatabase(embedding_model)\n",
        "pdf_vector_db.insert_pdf_content(documents, metadata)\n",
        "\n",
        "# Search PDF content\n",
        "results = pdf_vector_db.search_pdf_content(\"query\", k=3)\n",
        "```\n",
        "\n",
        "Your PDF RAG system is now ready for processing real PDF documents with enhanced metadata and search capabilities! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (2.3.2)\n",
            "Requirement already satisfied: openpyxl in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from pandas) (2.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Excel integration classes loaded successfully!\n",
            "Ready to process Excel files, PDFs, and text content in a unified RAG system!\n"
          ]
        }
      ],
      "source": [
        "# ACTIVITY 3: Excel Integration for Enhanced RAG\n",
        "\n",
        "# Install required packages for Excel processing\n",
        "#!pip install pandas openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "\n",
        "# Enhanced Excel Content Loader with URL Support and Metadata Extraction\n",
        "class ExcelContentLoader:\n",
        "    def __init__(self, url: str):\n",
        "        self.url = url\n",
        "        self.filename = url.split('/')[-1]\n",
        "        \n",
        "    def download_excel_file(self) -> BytesIO:\n",
        "        \"\"\"Download Excel file from URL\"\"\"\n",
        "        try:\n",
        "            print(f\"üìä Downloading Excel file from: {self.url}\")\n",
        "            response = requests.get(self.url)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            excel_buffer = BytesIO(response.content)\n",
        "            print(f\"‚úÖ Successfully downloaded Excel file: {self.filename}\")\n",
        "            return excel_buffer\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error downloading Excel file: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def extract_text_from_dataframe(self, df: pd.DataFrame, sheet_name: str) -> str:\n",
        "        \"\"\"Convert DataFrame to structured text format\"\"\"\n",
        "        text_content = f\"Sheet: {sheet_name}\\n\"\n",
        "        text_content += f\"Columns: {', '.join(df.columns.astype(str))}\\n\"\n",
        "        text_content += f\"Number of rows: {len(df)}\\n\\n\"\n",
        "        \n",
        "        # Add column descriptions\n",
        "        text_content += \"Column Data Types:\\n\"\n",
        "        for col in df.columns:\n",
        "            dtype = str(df[col].dtype)\n",
        "            non_null_count = df[col].count()\n",
        "            text_content += f\"- {col}: {dtype} ({non_null_count} non-null values)\\n\"\n",
        "        text_content += \"\\n\"\n",
        "        \n",
        "        # Add sample data (first few rows)\n",
        "        text_content += \"Sample Data (First 10 Rows):\\n\"\n",
        "        sample_df = df.head(10)\n",
        "        \n",
        "        for idx, row in sample_df.iterrows():\n",
        "            text_content += f\"Row {idx + 1}:\\n\"\n",
        "            for col in df.columns:\n",
        "                value = row[col]\n",
        "                if pd.notna(value):\n",
        "                    text_content += f\"  {col}: {value}\\n\"\n",
        "            text_content += \"\\n\"\n",
        "        \n",
        "        # Add summary statistics for numeric columns\n",
        "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "        if len(numeric_cols) > 0:\n",
        "            text_content += \"Numeric Column Statistics:\\n\"\n",
        "            for col in numeric_cols:\n",
        "                if df[col].count() > 0:\n",
        "                    mean_val = df[col].mean()\n",
        "                    min_val = df[col].min()\n",
        "                    max_val = df[col].max()\n",
        "                    text_content += f\"{col}: Mean={mean_val:.2f}, Min={min_val}, Max={max_val}\\n\"\n",
        "            text_content += \"\\n\"\n",
        "        \n",
        "        return text_content\n",
        "    \n",
        "    def load_excel_content(self) -> Tuple[List[str], List[Dict]]:\n",
        "        \"\"\"Load Excel content with comprehensive metadata\"\"\"\n",
        "        try:\n",
        "            excel_buffer = self.download_excel_file()\n",
        "            \n",
        "            # Read Excel file with all sheets\n",
        "            excel_file = pd.ExcelFile(excel_buffer)\n",
        "            sheet_names = excel_file.sheet_names\n",
        "            \n",
        "            print(f\"üìã Found {len(sheet_names)} sheets: {sheet_names}\")\n",
        "            \n",
        "            documents = []\n",
        "            metadata_list = []\n",
        "            \n",
        "            for sheet_name in sheet_names:\n",
        "                try:\n",
        "                    # Load sheet data\n",
        "                    df = pd.read_excel(excel_buffer, sheet_name=sheet_name)\n",
        "                    print(f\"üìä Processing sheet '{sheet_name}': {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "                    \n",
        "                    if df.empty:\n",
        "                        print(f\"‚ö†Ô∏è Sheet '{sheet_name}' is empty, skipping...\")\n",
        "                        continue\n",
        "                    \n",
        "                    # Extract text content\n",
        "                    text_content = self.extract_text_from_dataframe(df, sheet_name)\n",
        "                    documents.append(text_content)\n",
        "                    \n",
        "                    # Create comprehensive metadata\n",
        "                    sheet_metadata = {\n",
        "                        \"source\": \"Excel File\",\n",
        "                        \"url\": self.url,\n",
        "                        \"filename\": self.filename,\n",
        "                        \"sheet_name\": sheet_name,\n",
        "                        \"total_sheets\": len(sheet_names),\n",
        "                        \"sheet_index\": sheet_names.index(sheet_name),\n",
        "                        \"row_count\": df.shape[0],\n",
        "                        \"column_count\": df.shape[1],\n",
        "                        \"columns\": list(df.columns.astype(str)),\n",
        "                        \"data_types\": dict(df.dtypes.astype(str)),\n",
        "                        \"non_null_counts\": dict(df.count()),\n",
        "                        \"content_type\": \"Excel Spreadsheet\",\n",
        "                        \"extraction_method\": \"pandas\",\n",
        "                        \"has_numeric_data\": len(df.select_dtypes(include=['number']).columns) > 0,\n",
        "                        \"memory_usage_bytes\": df.memory_usage(deep=True).sum()\n",
        "                    }\n",
        "                    \n",
        "                    metadata_list.append(sheet_metadata)\n",
        "                    \n",
        "                except Exception as sheet_error:\n",
        "                    print(f\"‚ùå Error processing sheet '{sheet_name}': {sheet_error}\")\n",
        "                    continue\n",
        "            \n",
        "            print(f\"‚úÖ Successfully loaded {len(documents)} sheets from Excel file!\")\n",
        "            return documents, metadata_list\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading Excel content: {e}\")\n",
        "            raise\n",
        "\n",
        "# Enhanced Text Splitter for Excel Content with Row Context Preservation\n",
        "class ExcelTextSplitter:\n",
        "    def __init__(self, chunk_size: int = 400, overlap: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "    \n",
        "    def _extract_sheet_info_from_chunk(self, chunk: str) -> Dict:\n",
        "        \"\"\"Extract sheet information from a chunk of text\"\"\"\n",
        "        lines = chunk.split('\\n')\n",
        "        sheet_name = \"Unknown\"\n",
        "        columns = []\n",
        "        \n",
        "        for line in lines:\n",
        "            if line.startswith(\"Sheet: \"):\n",
        "                sheet_name = line.replace(\"Sheet: \", \"\").strip()\n",
        "            elif line.startswith(\"Columns: \"):\n",
        "                columns_str = line.replace(\"Columns: \", \"\").strip()\n",
        "                columns = [col.strip() for col in columns_str.split(',')]\n",
        "                break\n",
        "        \n",
        "        return {\"sheet_name\": sheet_name, \"columns\": columns}\n",
        "    \n",
        "    def split_excel_content(self, texts: List[str], metadata_list: List[Dict] = None) -> Tuple[List[str], List[Dict]]:\n",
        "        \"\"\"Split Excel content while preserving sheet and row context\"\"\"\n",
        "        split_texts = []\n",
        "        split_metadata = []\n",
        "        \n",
        "        for i, text in enumerate(texts):\n",
        "            base_metadata = metadata_list[i] if metadata_list and i < len(metadata_list) else {}\n",
        "            \n",
        "            # Split by sheet sections first to maintain context\n",
        "            sheet_sections = text.split('\\n\\n')\n",
        "            current_chunk = \"\"\n",
        "            chunk_num = 0\n",
        "            current_sheet = \"Unknown\"\n",
        "            \n",
        "            for section in sheet_sections:\n",
        "                if not section.strip():\n",
        "                    continue\n",
        "                    \n",
        "                # If this section starts with \"Sheet:\", it's a new sheet\n",
        "                if section.startswith(\"Sheet: \"):\n",
        "                    current_sheet = section.split('\\n')[0].replace(\"Sheet: \", \"\").strip()\n",
        "                \n",
        "                # If adding this section would exceed chunk size, start new chunk\n",
        "                if len(current_chunk + section) > self.chunk_size and current_chunk:\n",
        "                    # Extract sheet info from current chunk\n",
        "                    sheet_info = self._extract_sheet_info_from_chunk(current_chunk)\n",
        "                    \n",
        "                    split_texts.append(current_chunk.strip())\n",
        "                    \n",
        "                    # Enhanced metadata with sheet and row info\n",
        "                    chunk_metadata = base_metadata.copy()\n",
        "                    chunk_metadata.update({\n",
        "                        \"chunk_number\": chunk_num,\n",
        "                        \"chunk_length\": len(current_chunk),\n",
        "                        \"sheet_name\": sheet_info[\"sheet_name\"],\n",
        "                        \"columns\": sheet_info[\"columns\"],\n",
        "                        \"original_doc_index\": i,\n",
        "                        \"content_type\": \"Excel Spreadsheet Chunk\"\n",
        "                    })\n",
        "                    split_metadata.append(chunk_metadata)\n",
        "                    \n",
        "                    chunk_num += 1\n",
        "                    current_chunk = section  # Start new chunk\n",
        "                else:\n",
        "                    current_chunk += \"\\n\" + section if current_chunk else section\n",
        "            \n",
        "            # Don't forget the last chunk\n",
        "            if current_chunk.strip():\n",
        "                sheet_info = self._extract_sheet_info_from_chunk(current_chunk)\n",
        "                \n",
        "                split_texts.append(current_chunk.strip())\n",
        "                \n",
        "                chunk_metadata = base_metadata.copy()\n",
        "                chunk_metadata.update({\n",
        "                    \"chunk_number\": chunk_num,\n",
        "                    \"chunk_length\": len(current_chunk),\n",
        "                    \"sheet_name\": sheet_info[\"sheet_name\"],\n",
        "                    \"columns\": sheet_info[\"columns\"],\n",
        "                    \"original_doc_index\": i,\n",
        "                    \"content_type\": \"Excel Spreadsheet Chunk\"\n",
        "                })\n",
        "                split_metadata.append(chunk_metadata)\n",
        "        \n",
        "        return split_texts, split_metadata\n",
        "\n",
        "# Multi-Source RAG Pipeline that handles Excel, PDF, and Text\n",
        "class MultiSourceRAGPipeline:\n",
        "    def __init__(self, llm, vector_db_retriever, metadata_mapping: Dict = None, response_style: str = \"detailed\", include_scores: bool = False):\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "        self.metadata_mapping = metadata_mapping or {}\n",
        "        self.response_style = response_style\n",
        "        self.include_scores = include_scores\n",
        "    \n",
        "    def _format_source_context(self, context: str, metadata: Dict, source_num: int) -> str:\n",
        "        \"\"\"Format context with appropriate source information\"\"\"\n",
        "        content_type = metadata.get(\"content_type\", \"Unknown\")\n",
        "        \n",
        "        if \"Excel\" in content_type:\n",
        "            # Excel content with sheet info\n",
        "            sheet_name = metadata.get(\"sheet_name\", \"Unknown Sheet\")\n",
        "            columns = metadata.get(\"columns\", [])\n",
        "            column_info = f\" (Columns: {', '.join(columns[:3])}{', ...' if len(columns) > 3 else ''})\" if columns else \"\"\n",
        "            \n",
        "            return f\"[Source {source_num} - Excel Sheet: '{sheet_name}'{column_info}]: {context}\"\n",
        "        \n",
        "        elif \"PDF\" in content_type:\n",
        "            # PDF content with page info\n",
        "            chunk_num = metadata.get(\"chunk_number\", \"Unknown\")\n",
        "            return f\"[Source {source_num} - PDF Page {chunk_num}]: {context}\"\n",
        "        \n",
        "        else:\n",
        "            # Default text content\n",
        "            return f\"[Source {source_num} - Text]: {context}\"\n",
        "    \n",
        "    def run_pipeline(self, user_query: str, k: int = 4, **system_kwargs) -> dict:\n",
        "        # Retrieve relevant contexts (returns tuples of (text, score))\n",
        "        search_results = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
        "        \n",
        "        context_prompt = \"\"\n",
        "        similarity_scores = []\n",
        "        metadata_info = []\n",
        "        source_types = set()\n",
        "        \n",
        "        for i, (context, score) in enumerate(search_results, 1):\n",
        "            # Get metadata for this context from the mapping\n",
        "            metadata = self.metadata_mapping.get(context, {\n",
        "                \"content_type\": \"Unknown\",\n",
        "                \"source\": \"Document\"\n",
        "            })\n",
        "            \n",
        "            # Format context with appropriate source information\n",
        "            formatted_context = self._format_source_context(context, metadata, i)\n",
        "            context_prompt += f\"{formatted_context}\\n\\n\"\n",
        "            \n",
        "            similarity_scores.append(f\"Source {i}: {score:.3f}\")\n",
        "            metadata_info.append(metadata)\n",
        "            \n",
        "            # Track content types\n",
        "            content_type = metadata.get(\"content_type\", \"Text\")\n",
        "            source_types.add(content_type.split()[0])  # Get first word (Excel, PDF, etc.)\n",
        "        \n",
        "        # Create enhanced system message for multi-source content\n",
        "        enhanced_system_template = \"\"\"You are a knowledgeable assistant that answers questions based strictly on provided context from multiple sources.\n",
        "\n",
        "        Instructions:\n",
        "        - Only answer questions using information from the provided context\n",
        "        - If the context doesn't contain relevant information, respond with \"I don't know\"\n",
        "        - Be accurate and cite specific sources with sheet names (for Excel) or page numbers (for PDFs)\n",
        "        - When referencing Excel content, mention the sheet name and relevant columns\n",
        "        - When referencing PDF content, mention the page or section\n",
        "        - Keep responses {response_style} and {response_length}\n",
        "        - Only use the provided context. Do not use external knowledge.\n",
        "        - If information comes from multiple sources, clearly indicate this in your response\"\"\"\n",
        "        \n",
        "        # Format prompts\n",
        "        from aimakerspace.openai_utils.prompts import SystemRolePrompt, UserRolePrompt\n",
        "        \n",
        "        system_prompt = SystemRolePrompt(enhanced_system_template)\n",
        "        user_template = \"\"\"Context Information from Multiple Sources:\n",
        "{context}\n",
        "\n",
        "Number of sources found: {context_count}\n",
        "Source types: {source_types}\n",
        "Distance metric used: Euclidean\n",
        "{similarity_scores}\n",
        "\n",
        "Question: {user_query}\n",
        "\n",
        "Please provide your answer based solely on the context above, including specific source citations with sheet names/pages where possible.\"\"\"\n",
        "        \n",
        "        user_prompt = UserRolePrompt(user_template)\n",
        "        \n",
        "        system_params = {\n",
        "            \"response_style\": self.response_style,\n",
        "            \"response_length\": system_kwargs.get(\"response_length\", \"detailed\")\n",
        "        }\n",
        "        \n",
        "        user_params = {\n",
        "            \"user_query\": user_query,\n",
        "            \"context\": context_prompt.strip(),\n",
        "            \"context_count\": len(search_results),\n",
        "            \"source_types\": \", \".join(sorted(source_types)),\n",
        "            \"similarity_scores\": f\"Relevance scores: {', '.join(similarity_scores)}\" if self.include_scores else \"\"\n",
        "        }\n",
        "        \n",
        "        formatted_system_prompt = system_prompt.create_message(**system_params)\n",
        "        formatted_user_prompt = user_prompt.create_message(**user_params)\n",
        "\n",
        "        return {\n",
        "            \"response\": self.llm.run([formatted_system_prompt, formatted_user_prompt]), \n",
        "            \"context\": search_results,\n",
        "            \"context_count\": len(search_results),\n",
        "            \"similarity_scores\": similarity_scores,\n",
        "            \"metadata\": metadata_info,\n",
        "            \"source_types\": list(source_types),\n",
        "            \"distance_metric\": \"euclidean\",\n",
        "            \"embedding_model\": getattr(self.vector_db_retriever.embedding_model, 'embeddings_model_name', 'text-embedding-3-small'),\n",
        "            \"prompts_used\": {\n",
        "                \"system\": formatted_system_prompt,\n",
        "                \"user\": formatted_user_prompt\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Excel integration classes loaded successfully!\")\n",
        "print(\"Ready to process Excel files, PDFs, and text content in a unified RAG system!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Excel Integration Demo...\n",
            "Excel URL: https://www.exceldemy.com/wp-content/uploads/2023/12/Inventory-Records-Sample-Data.xlsx\n",
            "\n",
            "üìä Step 1: Loading Excel content...\n",
            "üìä Downloading Excel file from: https://www.exceldemy.com/wp-content/uploads/2023/12/Inventory-Records-Sample-Data.xlsx\n",
            "‚úÖ Successfully downloaded Excel file: Inventory-Records-Sample-Data.xlsx\n",
            "üìã Found 1 sheets: ['Inventory Records Data']\n",
            "üìä Processing sheet 'Inventory Records Data': 51 rows, 9 columns\n",
            "‚úÖ Successfully loaded 1 sheets from Excel file!\n",
            "‚úÖ Loaded 1 Excel sheets\n",
            "\n",
            "üî® Step 2: Splitting Excel content...\n",
            "‚úÖ Created 9 Excel chunks from 1 sheets\n",
            "\n",
            "üîó Step 3: Combining Excel with PDF content for multi-source RAG...\n",
            "üìä Total documents: 47 (38 PDF + 9 Excel)\n",
            "üìã Creating metadata mapping...\n",
            "‚úÖ Created metadata mapping for 47 documents\n",
            "\n",
            "üéØ Step 4: Building multi-source vector database...\n",
            "üîÑ Building embeddings for all documents...\n",
            "  Processing document 1/47\n",
            "  Processing document 2/47\n",
            "  Processing document 2/47\n",
            "  Processing document 3/47\n",
            "  Processing document 4/47\n",
            "  Processing document 3/47\n",
            "  Processing document 4/47\n",
            "  Processing document 5/47\n",
            "  Processing document 5/47\n",
            "  Processing document 6/47\n",
            "  Processing document 6/47\n",
            "  Processing document 7/47\n",
            "  Processing document 7/47\n",
            "  Processing document 8/47\n",
            "  Processing document 8/47\n",
            "  Processing document 9/47\n",
            "  Processing document 9/47\n",
            "  Processing document 10/47\n",
            "  Processing document 10/47\n",
            "  Processing document 11/47\n",
            "  Processing document 11/47\n",
            "  Processing document 12/47\n",
            "  Processing document 12/47\n",
            "  Processing document 13/47\n",
            "  Processing document 14/47\n",
            "  Processing document 13/47\n",
            "  Processing document 14/47\n",
            "  Processing document 15/47\n",
            "  Processing document 15/47\n",
            "  Processing document 16/47\n",
            "  Processing document 16/47\n",
            "  Processing document 17/47\n",
            "  Processing document 17/47\n",
            "  Processing document 18/47\n",
            "  Processing document 19/47\n",
            "  Processing document 18/47\n",
            "  Processing document 19/47\n",
            "  Processing document 20/47\n",
            "  Processing document 21/47\n",
            "  Processing document 20/47\n",
            "  Processing document 21/47\n",
            "  Processing document 22/47\n",
            "  Processing document 22/47\n",
            "  Processing document 23/47\n",
            "  Processing document 23/47\n",
            "  Processing document 24/47\n",
            "  Processing document 25/47\n",
            "  Processing document 24/47\n",
            "  Processing document 25/47\n",
            "  Processing document 26/47\n",
            "  Processing document 27/47\n",
            "  Processing document 26/47\n",
            "  Processing document 27/47\n",
            "  Processing document 28/47\n",
            "  Processing document 29/47\n",
            "  Processing document 28/47\n",
            "  Processing document 29/47\n",
            "  Processing document 30/47\n",
            "  Processing document 31/47\n",
            "  Processing document 30/47\n",
            "  Processing document 31/47\n",
            "  Processing document 32/47\n",
            "  Processing document 32/47\n",
            "  Processing document 33/47\n",
            "  Processing document 34/47\n",
            "  Processing document 33/47\n",
            "  Processing document 34/47\n",
            "  Processing document 35/47\n",
            "  Processing document 35/47\n",
            "  Processing document 36/47\n",
            "  Processing document 37/47\n",
            "  Processing document 36/47\n",
            "  Processing document 37/47\n",
            "  Processing document 38/47\n",
            "  Processing document 38/47\n",
            "  Processing document 39/47\n",
            "  Processing document 39/47\n",
            "  Processing document 40/47\n",
            "  Processing document 40/47\n",
            "  Processing document 41/47\n",
            "  Processing document 42/47\n",
            "  Processing document 41/47\n",
            "  Processing document 42/47\n",
            "  Processing document 43/47\n",
            "  Processing document 43/47\n",
            "  Processing document 44/47\n",
            "  Processing document 45/47\n",
            "  Processing document 44/47\n",
            "  Processing document 45/47\n",
            "  Processing document 46/47\n",
            "  Processing document 47/47\n",
            "  Processing document 46/47\n",
            "  Processing document 47/47\n",
            "‚úÖ Multi-source vector database created successfully!\n",
            "\n",
            "ü§ñ Step 5: Creating multi-source RAG pipeline...\n",
            "‚úÖ Multi-source RAG pipeline ready!\n",
            "\n",
            "üß™ Step 6: Testing multi-source RAG with sample queries...\n",
            "Running test queries...\n",
            "\n",
            "üîç Query 1: What information is available about inventory records?\n",
            "‚úÖ Multi-source vector database created successfully!\n",
            "\n",
            "ü§ñ Step 5: Creating multi-source RAG pipeline...\n",
            "‚úÖ Multi-source RAG pipeline ready!\n",
            "\n",
            "üß™ Step 6: Testing multi-source RAG with sample queries...\n",
            "Running test queries...\n",
            "\n",
            "üîç Query 1: What information is available about inventory records?\n",
            "üìä Sources found: 4\n",
            "üìà Source types: Excel\n",
            "üéØ Answer: The available information about inventory records is found mostly in the Excel sheets referenced under \"Inventory Records Data\" and other related unnamed sheets. Specifically:\n",
            "\n",
            "1. The sheet labeled 'Inventory Records Data' contains multiple columns, including unnamed columns from 0 to 8, with about ...\n",
            "--------------------------------------------------\n",
            "\n",
            "üîç Query 2: What data is contained in the Excel spreadsheet?\n",
            "üìä Sources found: 4\n",
            "üìà Source types: Excel\n",
            "üéØ Answer: The available information about inventory records is found mostly in the Excel sheets referenced under \"Inventory Records Data\" and other related unnamed sheets. Specifically:\n",
            "\n",
            "1. The sheet labeled 'Inventory Records Data' contains multiple columns, including unnamed columns from 0 to 8, with about ...\n",
            "--------------------------------------------------\n",
            "\n",
            "üîç Query 2: What data is contained in the Excel spreadsheet?\n",
            "üìä Sources found: 4\n",
            "üìà Source types: Excel\n",
            "üéØ Answer: The Excel spreadsheet contains inventory data records detailing product information and stock movement. Specifically, the sheet titled \"Inventory Records Data\" has 51 rows and includes these data columns (from source 3 and source 2):\n",
            "\n",
            "- Product ID\n",
            "- Product Name\n",
            "- Opening Stock\n",
            "- Purchase/Stock in\n",
            "-...\n",
            "--------------------------------------------------\n",
            "üéä Activity 2 Complete! Excel integration successful!\n",
            "‚ú® The system can now process Excel files, PDFs, and text content together!\n",
            "üìä Sources found: 4\n",
            "üìà Source types: Excel\n",
            "üéØ Answer: The Excel spreadsheet contains inventory data records detailing product information and stock movement. Specifically, the sheet titled \"Inventory Records Data\" has 51 rows and includes these data columns (from source 3 and source 2):\n",
            "\n",
            "- Product ID\n",
            "- Product Name\n",
            "- Opening Stock\n",
            "- Purchase/Stock in\n",
            "-...\n",
            "--------------------------------------------------\n",
            "üéä Activity 2 Complete! Excel integration successful!\n",
            "‚ú® The system can now process Excel files, PDFs, and text content together!\n"
          ]
        }
      ],
      "source": [
        "# Excel Integration Demo - Load and Process Excel File\n",
        "\n",
        "# Define the Excel file URL (inventory data)\n",
        "excel_url = \"https://www.exceldemy.com/wp-content/uploads/2023/12/Inventory-Records-Sample-Data.xlsx\"\n",
        "\n",
        "print(\"üöÄ Starting Excel Integration Demo...\")\n",
        "print(f\"Excel URL: {excel_url}\")\n",
        "\n",
        "# 1. Load Excel content\n",
        "print(\"\\nüìä Step 1: Loading Excel content...\")\n",
        "excel_loader = ExcelContentLoader(excel_url)\n",
        "excel_documents, excel_metadata = excel_loader.load_excel_content()\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(excel_documents)} Excel sheets\")\n",
        "\n",
        "# 2. Split Excel content into chunks\n",
        "print(\"\\nüî® Step 2: Splitting Excel content...\")\n",
        "excel_splitter = ExcelTextSplitter(chunk_size=300, overlap=50)  # Smaller chunks for Excel content\n",
        "excel_split_documents, excel_split_metadata = excel_splitter.split_excel_content(excel_documents, excel_metadata)\n",
        "\n",
        "print(f\"‚úÖ Created {len(excel_split_documents)} Excel chunks from {len(excel_documents)} sheets\")\n",
        "\n",
        "# 3. Combine with existing PDF content for multi-source RAG\n",
        "print(\"\\nüîó Step 3: Combining Excel with PDF content for multi-source RAG...\")\n",
        "\n",
        "# Combine all documents and metadata\n",
        "all_documents = pdf_split_documents + excel_split_documents\n",
        "all_metadata = pdf_split_metadata + excel_split_metadata\n",
        "\n",
        "print(f\"üìä Total documents: {len(all_documents)} ({len(pdf_split_documents)} PDF + {len(excel_split_documents)} Excel)\")\n",
        "\n",
        "# Create metadata mapping for the RAG pipeline\n",
        "print(\"üìã Creating metadata mapping...\")\n",
        "metadata_mapping = {}\n",
        "for doc, meta in zip(all_documents, all_metadata):\n",
        "    metadata_mapping[doc] = meta\n",
        "\n",
        "print(f\"‚úÖ Created metadata mapping for {len(metadata_mapping)} documents\")\n",
        "\n",
        "# 4. Create enhanced vector database with multi-source content\n",
        "print(\"\\nüéØ Step 4: Building multi-source vector database...\")\n",
        "# Use the existing VectorDatabase from aimakerspace\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "import asyncio\n",
        "import numpy as np\n",
        "\n",
        "multi_source_vector_db = VectorDatabase(embedding_model=embedding_model)\n",
        "\n",
        "# Build the vector database by inserting each document with its embedding\n",
        "print(\"üîÑ Building embeddings for all documents...\")\n",
        "for i, document in enumerate(all_documents):\n",
        "    print(f\"  Processing document {i+1}/{len(all_documents)}\")\n",
        "    # Get embedding for this document\n",
        "    embedding = embedding_model.get_embedding(document)\n",
        "    # Insert using the document text as key and embedding as vector\n",
        "    multi_source_vector_db.insert(document, np.array(embedding))\n",
        "\n",
        "print(\"‚úÖ Multi-source vector database created successfully!\")\n",
        "\n",
        "# 5. Create multi-source RAG pipeline\n",
        "print(\"\\nü§ñ Step 5: Creating multi-source RAG pipeline...\")\n",
        "multi_source_rag = MultiSourceRAGPipeline(\n",
        "    llm=chat_openai,\n",
        "    vector_db_retriever=multi_source_vector_db,\n",
        "    metadata_mapping=metadata_mapping,\n",
        "    response_style=\"detailed\",\n",
        "    include_scores=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Multi-source RAG pipeline ready!\")\n",
        "\n",
        "# 6. Test queries\n",
        "print(\"\\nüß™ Step 6: Testing multi-source RAG with sample queries...\")\n",
        "\n",
        "test_queries = [\n",
        "    \"What information is available about inventory records?\",\n",
        "    \"What data is contained in the Excel spreadsheet?\",\n",
        "    \"Can you analyze the inventory data from the Excel file?\",\n",
        "    \"What are the main topics discussed in the PDF?\",\n",
        "    \"Compare the information from both Excel and PDF sources\"\n",
        "]\n",
        "\n",
        "print(\"Running test queries...\")\n",
        "for i, query in enumerate(test_queries[:2], 1):  # Test first 2 queries\n",
        "    print(f\"\\nüîç Query {i}: {query}\")\n",
        "    result = multi_source_rag.run_pipeline(query, k=4)\n",
        "    \n",
        "    print(f\"üìä Sources found: {result['context_count']}\")\n",
        "    print(f\"üìà Source types: {', '.join(result['source_types'])}\")\n",
        "    print(f\"üéØ Answer: {result['response'][:300]}...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"üéä Activity 2 Complete! Excel integration successful!\")\n",
        "print(\"‚ú® The system can now process Excel files, PDFs, and text content together!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ude80 Enhanced RAG with Advanced Features\n",
        "\n",
        "This comprehensive enhancement includes **all three requested improvements** plus PDF support:\n",
        "\n",
        "### üéØ **Enhancement 1: Euclidean Distance Metric**\n",
        "- **Euclidean Distance**: Measures straight-line distance between vectors for better semantic matching\n",
        "- **Optimized**: Focused implementation using only the most effective distance metric\n",
        "- **Why it matters**: Euclidean distance often provides excellent results for high-dimensional embeddings\n",
        "\n",
        "### üìä **Enhancement 2: Metadata Support**\n",
        "- **Document Metadata**: File type, page count, extraction method\n",
        "- **Chunk Metadata**: Chunk number, position, length, source page\n",
        "- **Search Results**: Metadata returned with each retrieved context\n",
        "- **Citations**: Page references included in responses for better traceability\n",
        "\n",
        "### üß† **Enhancement 3: Different Embedding Model**\n",
        "- **text-embedding-3-large**: Upgraded from `text-embedding-3-small` for higher quality\n",
        "- **Custom Dimensions**: Using 512 dimensions instead of default 3072 (for efficiency)\n",
        "- **Better Semantics**: Larger model provides more nuanced understanding\n",
        "\n",
        "### üìÑ **Bonus: Advanced PDF Processing**\n",
        "- **Page-aware extraction**: Tracks which page content comes from\n",
        "- **Enhanced chunking**: Smart overlap and size control\n",
        "- **Metadata preservation**: Maintains context through the entire pipeline\n",
        "\n",
        "### üî¨ **Technical Improvements**\n",
        "- **EnhancedVectorDatabase**: Supports Euclidean distance and metadata storage\n",
        "- **MetadataTextSplitter**: Preserves and enhances metadata during text splitting\n",
        "- **EnhancedRAGPipeline**: Displays distance metric, model info, and metadata in results\n",
        "\n",
        "**The system now uses focused Euclidean distance for optimal semantic matching!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üèóÔ∏è Activity #3: Excel Integration\n",
        "\n",
        "Now let's add **Excel spreadsheet ingestion** capability to our enhanced RAG system! \n",
        "\n",
        "This will allow us to:\n",
        "- Extract data from Excel spreadsheets (.xlsx files)\n",
        "- Process spreadsheet metadata (sheets, columns, row counts)\n",
        "- Integrate Excel content with our existing PDF and text processing\n",
        "\n",
        "**Capabilities Added:**\n",
        "- **Excel Data Extraction**: Automatic spreadsheet reading and processing\n",
        "- **Sheet Metadata Capture**: Sheet names, column headers, data types\n",
        "- **Row-by-Row Processing**: Convert tabular data into searchable text"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv (3.13.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
