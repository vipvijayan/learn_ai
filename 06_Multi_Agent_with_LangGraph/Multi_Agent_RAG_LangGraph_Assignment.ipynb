{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxpWDFG11o3G"
   },
   "source": [
    "# Multi-Agent Workflows + RAG - LangGraph\n",
    "\n",
    "Today we'll be looking at an example of a Multi-Agent workflow that's powered by LangGraph, LCEL, and more!\n",
    "\n",
    "We're going to be, more specifically, looking at a \"heirarchical agent teams\" from the [AutoGen: Enabling Next-Gen LLM\n",
    "Applications via Multi-Agent Conversation](https://arxiv.org/pdf/2308.08155) paper.\n",
    "\n",
    "This will be the final \"graph\" of our system:\n",
    "\n",
    "![image](https://i.imgur.com/r5LMMCt.png)\n",
    "\n",
    "It's important to keep in mind that the actual implementation will be constructed of 3 separate graphs, the final one having 2 graphs as nodes! LangGraph is a heckuva tool!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyzoBrWoYeOZ"
   },
   "source": [
    "# ðŸ¤ BREAKOUT ROOM #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx3oaVoX5cA2"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpv2MWqu5vS9"
   },
   "source": [
    "Since we'll be relying on OpenAI's suite of models to power our agents today, we'll want to provide our OpenAI API Key.\n",
    "\n",
    "We're also going to be using the Tavily search tool - so we'll want to provide that API key as well!\n",
    "\n",
    "Instruction for how to obtain the Tavily API key can be found:\n",
    "\n",
    "1. [Tavily API Key](https://app.tavily.com/sign-in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h30OjkLfeR2Y",
    "outputId": "f75bb26e-b89d-4611-c29b-f339b3e868af"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_LD7rwT6PbO"
   },
   "source": [
    "## Task 1: Simple LangGraph RAG\n",
    "\n",
    "Now that we have our dependencies set-up - let's create a simple RAG graph that works over our AI Usage Data from previous sessions.\n",
    "\n",
    "> NOTE: While this particular example is very straight forward - you can \"plug in\" any complexity of chain you desire as a node in a LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JY7T5kxJ6jGn"
   },
   "source": [
    "### Retrieval\n",
    "\n",
    "The 'R' in 'RAG' - this is, at this point, fairly straightforward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGuPxSCk7Ztz"
   },
   "source": [
    "#### Data Collection and Processing\n",
    "\n",
    "A classic first step, at this point, let's grab our desired document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from arxiv) (6.0.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: sgmllib3k in /Users/vipinvijayan/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for Arxiv integration\n",
    "!pip install arxiv requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_m/_w2gs9cn34s17f99_9gyw7200000gn/T/ipykernel_87360/1129312266.py:35: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for paper in search.results():\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "Page request resulted in HTTP 503 (https://export.arxiv.org/api/query?search_query=artificial+intelligence+adoption+user+behavior+patterns+workplace&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:648\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    650\u001b[39m     HTTPError,\n\u001b[32m    651\u001b[39m     UnexpectedEmptyPageError,\n\u001b[32m    652\u001b[39m     requests.exceptions.ConnectionError,\n\u001b[32m    653\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:685\u001b[39m, in \u001b[36mClient.__try_parse_feed\u001b[39m\u001b[34m(self, url, first_page, try_index)\u001b[39m\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp.status_code != requests.codes.OK:\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(url, try_index, resp.status_code)\n\u001b[32m    687\u001b[39m feed = feedparser.parse(resp.content)\n",
      "\u001b[31mHTTPError\u001b[39m: Page request resulted in HTTP 503 (https://export.arxiv.org/api/query?search_query=artificial+intelligence+adoption+user+behavior+patterns+workplace&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:648\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    650\u001b[39m     HTTPError,\n\u001b[32m    651\u001b[39m     UnexpectedEmptyPageError,\n\u001b[32m    652\u001b[39m     requests.exceptions.ConnectionError,\n\u001b[32m    653\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:685\u001b[39m, in \u001b[36mClient.__try_parse_feed\u001b[39m\u001b[34m(self, url, first_page, try_index)\u001b[39m\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp.status_code != requests.codes.OK:\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(url, try_index, resp.status_code)\n\u001b[32m    687\u001b[39m feed = feedparser.parse(resp.content)\n",
      "\u001b[31mHTTPError\u001b[39m: Page request resulted in HTTP 503 (https://export.arxiv.org/api/query?search_query=artificial+intelligence+adoption+user+behavior+patterns+workplace&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:648\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    650\u001b[39m     HTTPError,\n\u001b[32m    651\u001b[39m     UnexpectedEmptyPageError,\n\u001b[32m    652\u001b[39m     requests.exceptions.ConnectionError,\n\u001b[32m    653\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:685\u001b[39m, in \u001b[36mClient.__try_parse_feed\u001b[39m\u001b[34m(self, url, first_page, try_index)\u001b[39m\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp.status_code != requests.codes.OK:\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(url, try_index, resp.status_code)\n\u001b[32m    687\u001b[39m feed = feedparser.parse(resp.content)\n",
      "\u001b[31mHTTPError\u001b[39m: Page request resulted in HTTP 503 (https://export.arxiv.org/api/query?search_query=artificial+intelligence+adoption+user+behavior+patterns+workplace&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m downloaded_files\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Try a different but related query - focusing on AI adoption and user behavior patterns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m downloaded_papers = \u001b[43msearch_and_download_arxiv_paper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43martificial intelligence adoption user behavior patterns workplace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     64\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully downloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(downloaded_papers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m paper(s)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# If no papers found with the first query, try alternative queries\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36msearch_and_download_arxiv_paper\u001b[39m\u001b[34m(query, max_results, download_dir)\u001b[39m\n\u001b[32m     27\u001b[39m search = arxiv.Search(\n\u001b[32m     28\u001b[39m     query=query,\n\u001b[32m     29\u001b[39m     max_results=max_results,\n\u001b[32m     30\u001b[39m     sort_by=arxiv.SortCriterion.Relevance\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m downloaded_files = []\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Generate filename from paper title\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafe_title\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m.\u001b[49m\u001b[43misalnum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:601\u001b[39m, in \u001b[36mClient._results\u001b[39m\u001b[34m(self, search, offset)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, search: Search, offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m) -> Generator[Result, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m    600\u001b[39m     page_url = \u001b[38;5;28mself\u001b[39m._format_url(search, offset, \u001b[38;5;28mself\u001b[39m.page_size)\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     feed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feed.entries:\n\u001b[32m    603\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mGot empty first page; stopping generation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:656\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _try_index < \u001b[38;5;28mself\u001b[39m.num_retries:\n\u001b[32m    655\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, _try_index, err)\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, _try_index, err)\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:656\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _try_index < \u001b[38;5;28mself\u001b[39m.num_retries:\n\u001b[32m    655\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, _try_index, err)\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, _try_index, err)\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:656\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _try_index < \u001b[38;5;28mself\u001b[39m.num_retries:\n\u001b[32m    655\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, _try_index, err)\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, _try_index, err)\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:658\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_feed(url, first_page=first_page, _try_index=_try_index + \u001b[32m1\u001b[39m)\n\u001b[32m    657\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, _try_index, err)\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:648\u001b[39m, in \u001b[36mClient._parse_feed\u001b[39m\u001b[34m(self, url, first_page, _try_index)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03mFetches the specified URL and parses it with feedparser.\u001b[39;00m\n\u001b[32m    643\u001b[39m \n\u001b[32m    644\u001b[39m \u001b[33;03mIf a request fails or is unexpectedly empty, retries the request up to\u001b[39;00m\n\u001b[32m    645\u001b[39m \u001b[33;03m`self.num_retries` times.\u001b[39;00m\n\u001b[32m    646\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    650\u001b[39m     HTTPError,\n\u001b[32m    651\u001b[39m     UnexpectedEmptyPageError,\n\u001b[32m    652\u001b[39m     requests.exceptions.ConnectionError,\n\u001b[32m    653\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    654\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _try_index < \u001b[38;5;28mself\u001b[39m.num_retries:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/myenv/lib/python3.13/site-packages/arxiv/__init__.py:685\u001b[39m, in \u001b[36mClient.__try_parse_feed\u001b[39m\u001b[34m(self, url, first_page, try_index)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28mself\u001b[39m._last_request_dt = datetime.now()\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp.status_code != requests.codes.OK:\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(url, try_index, resp.status_code)\n\u001b[32m    687\u001b[39m feed = feedparser.parse(resp.content)\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feed.entries) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first_page:\n",
      "\u001b[31mHTTPError\u001b[39m: Page request resulted in HTTP 503 (https://export.arxiv.org/api/query?search_query=artificial+intelligence+adoption+user+behavior+patterns+workplace&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100)"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "def search_and_download_arxiv_paper(\n",
    "    query: str = \"artificial intelligence usage\", \n",
    "    max_results: int = 1,\n",
    "    download_dir: str = \"data\"\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for and download papers from Arxiv\n",
    "    \n",
    "    Args:\n",
    "        query: Search query for Arxiv papers\n",
    "        max_results: Maximum number of papers to download\n",
    "        download_dir: Directory to save downloaded papers\n",
    "    \n",
    "    Returns:\n",
    "        List of file paths of downloaded papers\n",
    "    \"\"\"\n",
    "    # Create download directory if it doesn't exist\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    # Search for papers\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    \n",
    "    downloaded_files = []\n",
    "    \n",
    "    for paper in search.results():\n",
    "        try:\n",
    "            # Generate filename from paper title\n",
    "            safe_title = \"\".join(c for c in paper.title if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "            safe_title = safe_title.replace(' ', '_')[:50]  # Limit filename length\n",
    "            filename = f\"{safe_title}.pdf\"\n",
    "            filepath = os.path.join(download_dir, filename)\n",
    "            \n",
    "            # Download the paper\n",
    "            paper.download_pdf(dirpath=download_dir, filename=filename)\n",
    "            downloaded_files.append(filepath)\n",
    "            \n",
    "            print(f\"Downloaded: {paper.title}\")\n",
    "            print(f\"Authors: {', '.join([author.name for author in paper.authors])}\")\n",
    "            print(f\"Published: {paper.published.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"Summary: {paper.summary[:200]}...\")\n",
    "            print(f\"File saved as: {filepath}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading paper '{paper.title}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "# Try a different but related query - focusing on AI adoption and user behavior patterns\n",
    "downloaded_papers = search_and_download_arxiv_paper(\n",
    "    query=\"artificial intelligence adoption user behavior patterns workplace\", \n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "print(f\"Successfully downloaded {len(downloaded_papers)} paper(s)\")\n",
    "\n",
    "# If no papers found with the first query, try alternative queries\n",
    "if not downloaded_papers:\n",
    "    print(\"Trying alternative search queries...\")\n",
    "    \n",
    "    alternative_queries = [\n",
    "        \"machine learning user experience human factors\",\n",
    "        \"AI tool adoption workplace productivity\",\n",
    "        \"artificial intelligence user interface design\",\n",
    "        \"human AI interaction patterns\",\n",
    "        \"AI technology acceptance model\"\n",
    "    ]\n",
    "    \n",
    "    for alt_query in alternative_queries:\n",
    "        print(f\"Trying query: {alt_query}\")\n",
    "        downloaded_papers = search_and_download_arxiv_paper(\n",
    "            query=alt_query,\n",
    "            max_results=1\n",
    "        )\n",
    "        if downloaded_papers:\n",
    "            print(f\"Success with query: {alt_query}\")\n",
    "            break\n",
    "        \n",
    "if not downloaded_papers:\n",
    "    print(\"No papers could be downloaded. You may need to check your internet connection or try different search terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfuoEYRCln3H"
   },
   "outputs": [],
   "source": [
    "# Replace the hardcoded document loading with dynamic Arxiv paper loading\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Use the dynamically downloaded papers instead of hardcoded ones\n",
    "directory_loader = DirectoryLoader(\"data\", glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "\n",
    "# Load the dynamically fetched documents\n",
    "how_people_use_ai_documents = directory_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(how_people_use_ai_documents)} document(s) from Arxiv\")\n",
    "if how_people_use_ai_documents:\n",
    "    print(f\"First document preview: {how_people_use_ai_documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_t_F1zG6vXa"
   },
   "source": [
    "Now we can chunk it down to size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R7A_z8CgL79"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "\n",
    "how_people_use_ai_chunks = text_splitter.split_documents(how_people_use_ai_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGE-VuMc7AKv"
   },
   "source": [
    "Now we've successfully split our single PDF into..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgYBHsdWmLvW",
    "outputId": "aa9a830e-f7db-4bb3-f542-c0614cb01aca"
   },
   "outputs": [],
   "source": [
    "len(how_people_use_ai_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGWs7KTd7QPS"
   },
   "source": [
    "#### Embedding Model and Vector Store\n",
    "\n",
    "Now that we have our chunked document - lets create a vector store, which will first require us to create an embedding model to get the vector representations of our text!\n",
    "\n",
    "We'll use OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) model - as it's cheap, and performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLIWMMZCmfrj"
   },
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTEi7Ww573sc"
   },
   "source": [
    "Now we can create our QDrant backed vector store!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xct51f8omVAU"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "qdrant_vectorstore = Qdrant.from_documents(\n",
    "    documents=how_people_use_ai_chunks,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzGq6o4s79Ar"
   },
   "source": [
    "Let's make sure we can access it as a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTnQZbWymi4K"
   },
   "outputs": [],
   "source": [
    "qdrant_retriever = qdrant_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU8qSrMS7_D7"
   },
   "source": [
    "### Augmented\n",
    "\n",
    "Now that we have our retrieval process set-up, we need to set up our \"augmentation\" process - AKA a prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lezTN0zCmk46"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "HUMAN_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context respond with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", HUMAN_TEMPLATE)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9fa63nM7IKK"
   },
   "source": [
    "### Generation\n",
    "\n",
    "Last, but certainly not least, let's put the 'G' in 'RAG' by adding our generator - in this case, we can rely on OpenAI's [`gpt-4o-mini`](https://platform.openai.com/docs/models/gpt-4o-mini) model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwEi29-Jo3a8"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qO-ZC0T98XJJ"
   },
   "source": [
    "### RAG - Retrieval Augmented Generation\n",
    "\n",
    "All that's left to do is combine our R, A, and G into a single graph - and we're off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlOJrPm_oT3S"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class State(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document]\n",
    "  response: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "  retrieved_docs = qdrant_retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "  generator_chain = chat_prompt | generator_llm | StrOutputParser()\n",
    "  response = generator_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
    "  return {\"response\" : response}\n",
    "\n",
    "rag_graph = StateGraph(State).add_sequence([retrieve, generate])\n",
    "rag_graph.add_edge(START, \"retrieve\")\n",
    "compiled_rag_graph = rag_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiWrbXpu8ggz"
   },
   "source": [
    "Let's test this out and make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "gJhFlW32pBPe",
    "outputId": "7aee04b6-608f-4639-adca-66225d4d3002"
   },
   "outputs": [],
   "source": [
    "compiled_rag_graph.invoke({\"question\" : \"How does the average person use AI?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gReMizYk8qd-"
   },
   "source": [
    "### RAG Limitation\n",
    "\n",
    "Notice how we're hard-coding our data, while this is simply meant to be an illustrative example - you could easily extend this to work with any provied paper or document in order to have a more dynamic system.\n",
    "\n",
    "For now, we'll stick with this single hard-coded example in order to keep complexity down in an already very long notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxkbuir-H5rE"
   },
   "source": [
    "##### ðŸ—ï¸ Activity #1 (Bonus Marks)\n",
    "\n",
    "Allow the system to dynamically fetch Arxiv papers instead of hard coding them.\n",
    "\n",
    "> HINT: Tuesday's assignment will be very useful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U6a_pqQ9uWf"
   },
   "source": [
    "## Task 2: Helper Functions for Agent Graphs\n",
    "\n",
    "We'll be using a number of agents, nodes, and supervisors in the rest of the notebook - and so it will help to have a collection of useful helper functions that we can leverage to make our lives easier going forward.\n",
    "\n",
    "Let's start with the most simple one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDUnpEEl-L_F"
   },
   "source": [
    "#### Import Wall\n",
    "\n",
    "Here's a wall of imports we'll be needing going forward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbzoL3Q3-SG1"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Optional, TypedDict, Union\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb6Z3EEz-Asi"
   },
   "source": [
    "### Agent Node Helper\n",
    "\n",
    "Since we're going to be wrapping each of our agents into a node - it will help to have an easy way to create the node!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IF7KWfS-JKd"
   },
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwND2teK-WHm"
   },
   "source": [
    "### Agent Creation Helper Function\n",
    "\n",
    "Since we know we'll need to create agents to populate our agent nodes, let's use a helper function for that as well!\n",
    "\n",
    "Notice a few things:\n",
    "\n",
    "1. We have a standard suffix to append to our system messages for each agent to handle the tool calling and boilerplate prompting.\n",
    "2. Each agent has its our scratchpad.\n",
    "3. We're relying on OpenAI's function-calling API for tool selection\n",
    "4. Each agent is its own executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxLyHJt5-eUx"
   },
   "outputs": [],
   "source": [
    "def create_agent(\n",
    "    llm: ChatOpenAI,\n",
    "    tools: list,\n",
    "    system_prompt: str,\n",
    ") -> str:\n",
    "    \"\"\"Create a function-calling agent and add it to the graph.\"\"\"\n",
    "    system_prompt += (\"\\nWork autonomously according to your specialty, using the tools available to you.\"\n",
    "    \" Do not ask for clarification.\"\n",
    "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\"\n",
    "    \" You are chosen for a reason!\")\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6kmlR9d-1K5"
   },
   "source": [
    "### Supervisor Helper Function\n",
    "\n",
    "Finally, we need a \"supervisor\" that decides and routes tasks to specific agents.\n",
    "\n",
    "Since each \"team\" will have a collection of potential agents - this \"supervisor\" will act as an \"intelligent\" router to make sure that the right agent is selected for the right task.\n",
    "\n",
    "Notice that, at the end of the day, this \"supervisor\" is simply directing who acts next - or if the state is considered \"done\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2MXA83mrYE2"
   },
   "outputs": [],
   "source": [
    "def create_team_supervisor(llm: ChatOpenAI, system_prompt, members) -> str:\n",
    "    \"\"\"An LLM-based router.\"\"\"\n",
    "    options = [\"FINISH\"] + members\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next?\"\n",
    "                \" Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options), team_members=\", \".join(members))\n",
    "    return (\n",
    "        prompt\n",
    "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd0zfyq48jKb"
   },
   "source": [
    "## Task 3: Research Team - A LangGraph for Researching AI Usage Policy\n",
    "\n",
    "Now that we have our RAG chain set-up and some awesome helper functions, we want to create a LangGraph related to researching a specific topic, in this case: How People Use AI!\n",
    "\n",
    "We're going to start by equipping our Research Team with a few tools:\n",
    "\n",
    "1. Tavily Search - aka \"Google\", for the most up to date information possible.\n",
    "2. Our RAG chain - specific and high quality information about our topic.\n",
    "\n",
    "Let's create those tools now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNsVTZrH_alw"
   },
   "source": [
    "### Tool Creation\n",
    "\n",
    "As you can see below, some tools already come pre-packaged ready to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce7FKTZDgAWG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIR7cbTL9agM"
   },
   "source": [
    "Creating a custom tool, however, is very straightforward.\n",
    "\n",
    "> NOTE: You *must* include a docstring, as that is what the LLM will consider when deciding when to use this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSwO2L_UqFhm"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Tuple, Union\n",
    "from langchain_core.tools import tool\n",
    "import arxiv\n",
    "\n",
    "@tool\n",
    "def retrieve_information(\n",
    "    query: Annotated[str, \"query to ask the retrieve information tool\"]\n",
    "    ):\n",
    "  \"\"\"Use Retrieval Augmented Generation to retrieve information about how people use AI\"\"\"\n",
    "  return compiled_rag_graph.invoke({\"question\" : query})\n",
    "\n",
    "@tool\n",
    "def search_arxiv_papers(\n",
    "    query: Annotated[str, \"Search query for Arxiv papers\"],\n",
    "    max_results: Annotated[int, \"Maximum number of papers to search\"] = 3\n",
    ") -> Annotated[str, \"Information about found papers\"]:\n",
    "    \"\"\"Search for academic papers on Arxiv and return summaries\"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for paper in search.results():\n",
    "        paper_info = {\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": [author.name for author in paper.authors],\n",
    "            \"published\": paper.published.strftime('%Y-%m-%d'),\n",
    "            \"summary\": paper.summary[:300] + \"...\" if len(paper.summary) > 300 else paper.summary,\n",
    "            \"url\": paper.entry_id\n",
    "        }\n",
    "        results.append(paper_info)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No papers found for the given query.\"\n",
    "    \n",
    "    formatted_results = []\n",
    "    for i, paper in enumerate(results, 1):\n",
    "        formatted_results.append(\n",
    "            f\"{i}. **{paper['title']}**\\n\"\n",
    "            f\"   Authors: {', '.join(paper['authors'])}\\n\"\n",
    "            f\"   Published: {paper['published']}\\n\"\n",
    "            f\"   Summary: {paper['summary']}\\n\"\n",
    "            f\"   URL: {paper['url']}\\n\"\n",
    "        )\n",
    "    \n",
    "    return \"\\n\".join(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIlLPxj7Atpj"
   },
   "outputs": [],
   "source": [
    "search_agent = create_agent(\n",
    "    research_llm,\n",
    "    [tavily_tool, search_arxiv_papers],\n",
    "    \"You are a research assistant who can search for up-to-date info using the tavily search engine and academic papers from Arxiv.\",\n",
    ")\n",
    "search_node = functools.partial(agent_node, agent=search_agent, name=\"Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emLtesudA9Dd"
   },
   "source": [
    "#### Research Team: RAG Agent Node\n",
    "\n",
    "Now we can wrap our LCEL RAG pipeline in an agent node as well, using the LCEL RAG pipeline as the tool, as created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-nnAG9XA_p7"
   },
   "outputs": [],
   "source": [
    "research_agent = create_agent(\n",
    "    research_llm,\n",
    "    [retrieve_information],\n",
    "    \"You are a research assistant who can provide specific information on how people use AI\",\n",
    ")\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"HowPeopleUseAIRetriever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dA5z6T1CBeSc"
   },
   "source": [
    "### Research Team Supervisor Agent\n",
    "\n",
    "Notice that we're not yet creating our supervisor *node*, simply the agent here.\n",
    "\n",
    "Also notice how we need to provide a few extra pieces of information - including which tools we're using.\n",
    "\n",
    "> NOTE: It's important to use the *exact* tool name, as that is how the LLM will reference the tool. Also, it's important that your tool name is all a single alphanumeric string!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0g8CQMBrtFs"
   },
   "outputs": [],
   "source": [
    "research_supervisor_agent = create_team_supervisor(\n",
    "    research_llm,\n",
    "    (\"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  Search, HowPeopleUseAIRetriever. Given the following user request,\"\n",
    "    \" determine the subject to be researched and respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. \"\n",
    "    \" The Search agent can access both web search and academic papers from Arxiv.\"\n",
    "    \" You should never ask your team to do anything beyond research. They are not required to write content or posts.\"\n",
    "    \" You should only pass tasks to workers that are specifically research focused.\"\n",
    "    \" When finished, respond with FINISH.\"),\n",
    "    [\"Search\", \"HowPeopleUseAIRetriever\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qohn0DcgB_U1"
   },
   "source": [
    "### Research Team Graph Creation\n",
    "\n",
    "Now that we have our research team agent nodes created, and our supervisor agent - let's finally construct our graph!\n",
    "\n",
    "We'll start by creating our base graph from our state, and then adding the nodes/agent we've created as nodes on our LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0s2GAgJCN8G"
   },
   "outputs": [],
   "source": [
    "research_graph = StateGraph(ResearchTeamState)\n",
    "\n",
    "research_graph.add_node(\"Search\", search_node)\n",
    "research_graph.add_node(\"HowPeopleUseAIRetriever\", research_node)\n",
    "research_graph.add_node(\"ResearchSupervisor\", research_supervisor_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33qixRGNCaAX"
   },
   "source": [
    "Now we can define our edges - include our conditional edge from our supervisor to our agent nodes.\n",
    "\n",
    "Notice how we're always routing our agent nodes back to our supervisor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYSJIhijsGyg"
   },
   "outputs": [],
   "source": [
    "research_graph.add_edge(\"Search\", \"ResearchSupervisor\") \n",
    "research_graph.add_edge(\"HowPeopleUseAIRetriever\", \"ResearchSupervisor\")\n",
    "research_graph.add_conditional_edges(\n",
    "    \"ResearchSupervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\"Search\": \"Search\", \"HowPeopleUseAIRetriever\": \"HowPeopleUseAIRetriever\", \"FINISH\": END},\n",
    ")\n",
    "research_graph.set_entry_point(\"ResearchSupervisor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgGcuZzkCj1-"
   },
   "source": [
    "Now we can set our supervisor node as the entry point, and compile our graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1l-1I2Z3CnPX"
   },
   "outputs": [],
   "source": [
    "compiled_research_graph = research_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDwQpYTSEY13"
   },
   "source": [
    "#### Display Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_research_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfRvA2QfCqFL"
   },
   "source": [
    "The next part is key - since we need to \"wrap\" our LangGraph in order for it to be compatible in the following steps - let's create an LCEL chain out of it!\n",
    "\n",
    "This allows us to \"broadcast\" messages down to our Research Team LangGraph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1G7hmEINCx3i"
   },
   "outputs": [],
   "source": [
    "def enter_research_chain(message: str):\n",
    "    results = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "    }\n",
    "    return results\n",
    "\n",
    "research_chain = enter_research_chain | compiled_research_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGdoCdXWC7Pi"
   },
   "source": [
    "Now, finally, we can take it for a spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIDpFIg2sRUl",
    "outputId": "bb3803d4-5b32-4b0a-c8a1-1a1917425812"
   },
   "outputs": [],
   "source": [
    "for s in research_chain.stream(\n",
    "    \"How are people using AI to improve their lives?\", {\"recursion_limit\": 100}\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHAgsbwIIhwj"
   },
   "source": [
    "##### ðŸ—ï¸ Activity #2:\n",
    "\n",
    "Using whatever drawing application you wish - please label the flow above on a diagram of your graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH70eHGlJbq4"
   },
   "source": [
    "##### â“ Question #2:\n",
    "\n",
    "How could you make sure your Agent uses specific tools that you wish it to use? Are there any ways to concretely set a flow through tools?\n",
    "\n",
    "##### âœ… Answer:\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iktcBorGXmAW"
   },
   "source": [
    "# ðŸ¤ BREAKOUT ROOM #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejsHCZZ2EmwM"
   },
   "source": [
    "## Task 4: Document Writing Team - A LangGraph for Planning, Writing, and Editing a Formal Research Resport.\n",
    "\n",
    "Let's run it all back, this time specifically creating tools, agent nodes, and a graph for Planning, Writing, and Editing a Formal Research Resport!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Cohort Use Case Data\n",
    "\n",
    "Let's add a retriever for [previous cohort use-case data](./data/AIE7_Projects_with_Domains.csv) here!\n",
    "\n",
    "This will allow our response writing team reference previous responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "previous_cohort_loader = CSVLoader(\"data/AIE7_Projects_with_Domains.csv\", content_columns=[\"Project Domain\", \"Secondary Domain (if any)\"])\n",
    "previous_cohort = previous_cohort_loader.load()\n",
    "previous_cohort[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_previous_cohort_vectorstore = Qdrant.from_documents(\n",
    "    documents=previous_cohort,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_previous_cohort_retriever = qdrant_previous_cohort_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4awQtZ-oFUN-"
   },
   "source": [
    "### Tool Creation\n",
    "\n",
    "Let's create some tools that will help us understand, open, work with, and edit documents to our liking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptXilgparOkq"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Dict, Optional\n",
    "from typing_extensions import TypedDict\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "os.makedirs('./content/data', exist_ok=True)\n",
    "\n",
    "def create_random_subdirectory():\n",
    "    random_id = str(uuid.uuid4())[:8]  # Use first 8 characters of a UUID\n",
    "    subdirectory_path = os.path.join('./content/data', random_id)\n",
    "    os.makedirs(subdirectory_path, exist_ok=True)\n",
    "    return subdirectory_path\n",
    "\n",
    "WORKING_DIRECTORY = Path(create_random_subdirectory())\n",
    "\n",
    "@tool\n",
    "def create_outline(\n",
    "    points: Annotated[List[str], \"List of main points or sections.\"],\n",
    "    file_name: Annotated[str, \"File path to save the outline.\"],\n",
    ") -> Annotated[str, \"Path of the saved outline file.\"]:\n",
    "    \"\"\"Create and save an outline.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        for i, point in enumerate(points):\n",
    "            file.write(f\"{i + 1}. {point}\\n\")\n",
    "    return f\"Outline saved to {file_name}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def read_document(\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    "    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n",
    "    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",
    ") -> str:\n",
    "    \"\"\"Read the specified document.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    if start is not None:\n",
    "        start = 0\n",
    "    return \"\\n\".join(lines[start:end])\n",
    "\n",
    "@tool\n",
    "def write_document(\n",
    "    content: Annotated[str, \"Text content to be written into the document.\"],\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    ") -> Annotated[str, \"Path of the saved document file.\"]:\n",
    "    \"\"\"Create and save a text document.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.write(content)\n",
    "    return f\"Document saved to {file_name}\"\n",
    "\n",
    "### Previous Cohort Use Case Data\n",
    "@tool \n",
    "def reference_previous_responses(\n",
    "    query: Annotated[str, \"The query to search for in the previous responses.\"],\n",
    ") -> Annotated[str, \"The previous responses that match the query.\"]:\n",
    "    \"\"\"Search for previous responses that match the query.\"\"\"\n",
    "    return qdrant_previous_cohort_retriever.invoke(query)\n",
    "\n",
    "\n",
    "@tool\n",
    "def edit_document(\n",
    "    file_name: Annotated[str, \"Path of the document to be edited.\"],\n",
    "    inserts: Annotated[\n",
    "        Dict[int, str],\n",
    "        \"Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.\",\n",
    "    ] = {},\n",
    ") -> Annotated[str, \"Path of the edited document file.\"]:\n",
    "    \"\"\"Edit a document by inserting text at specific line numbers.\"\"\"\n",
    "\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sorted_inserts = sorted(inserts.items())\n",
    "\n",
    "    for line_number, text in sorted_inserts:\n",
    "        if 1 <= line_number <= len(lines) + 1:\n",
    "            lines.insert(line_number - 1, text + \"\\n\")\n",
    "        else:\n",
    "            return f\"Error: Line number {line_number} is out of range.\"\n",
    "\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "    return f\"Document edited and saved to {file_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8yH1IAYK7nL"
   },
   "source": [
    "##### ðŸ—ï¸ Activity #3:\n",
    "\n",
    "Describe, briefly, what each of these tools is doing in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "â€¢ create_outline: Makes a numbered list and saves it as a file\n",
    "\n",
    "â€¢ read_document: Opens a file and shows you what's inside\n",
    "\n",
    "â€¢ write_document: Creates a new file with text you give it\n",
    "\n",
    "â€¢ edit_document: Adds new text to an existing file at specific spots\n",
    "\n",
    "â€¢ reference_previous_responses: Looks through old responses for similar examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__Jw_XBIFwwa"
   },
   "source": [
    "### Document Writing State\n",
    "\n",
    "Just like with our Research Team state - we want to keep track of a few things, however this time - we also want to keep track of which files we've created - so let's add that here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoU2YwJRu7wD"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from pathlib import Path\n",
    "\n",
    "class DocWritingState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    team_members: str\n",
    "    next: str\n",
    "    current_files: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p1kQShmGHCh"
   },
   "source": [
    "### Document Writing Prelude Function\n",
    "\n",
    "Since we have a working directory - we want to be clear about what our current working directory looks like - this helper function will allow us to do that cleanly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G79mUggQGLVq"
   },
   "outputs": [],
   "source": [
    "def prelude(state):\n",
    "    written_files = []\n",
    "    if not WORKING_DIRECTORY.exists():\n",
    "        WORKING_DIRECTORY.mkdir()\n",
    "    try:\n",
    "        written_files = [\n",
    "            f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob(\"*\")\n",
    "        ]\n",
    "    except:\n",
    "        pass\n",
    "    if not written_files:\n",
    "        return {**state, \"current_files\": \"No files written.\"}\n",
    "    return {\n",
    "        **state,\n",
    "        \"current_files\": \"\\nBelow are files your team has written to the directory:\\n\"\n",
    "        + \"\\n\".join([f\" - {f}\" for f in written_files]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbSre9agT9Gb"
   },
   "source": [
    "### Document Writing Node Creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authoring_llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7oso327T_wa"
   },
   "outputs": [],
   "source": [
    "doc_writer_agent = create_agent(\n",
    "    authoring_llm,\n",
    "    [write_document, edit_document, read_document],\n",
    "    (\"You are an expert writing customer assistance responses.\\n\"\n",
    "    \"Below are files currently in your directory:\\n{current_files}\"),\n",
    ")\n",
    "context_aware_doc_writer_agent = prelude | doc_writer_agent\n",
    "doc_writing_node = functools.partial(\n",
    "    agent_node, agent=context_aware_doc_writer_agent, name=\"DocWriter\"\n",
    ")\n",
    "\n",
    "note_taking_agent = create_agent(\n",
    "    authoring_llm,\n",
    "    [create_outline, read_document, reference_previous_responses],\n",
    "    (\"You are an expert senior researcher tasked with writing a customer assistance outline and\"\n",
    "    \" taking notes to craft a customer assistance response.\\n{current_files}\"),\n",
    ")\n",
    "context_aware_note_taking_agent = prelude | note_taking_agent\n",
    "note_taking_node = functools.partial(\n",
    "    agent_node, agent=context_aware_note_taking_agent, name=\"NoteTaker\"\n",
    ")\n",
    "\n",
    "copy_editor_agent = create_agent(\n",
    "    authoring_llm,\n",
    "    [write_document, edit_document, read_document],\n",
    "    (\"You are an expert copy editor who focuses on fixing grammar, spelling, and tone issues\\n\"\n",
    "    \"Below are files currently in your directory:\\n{current_files}\"),\n",
    ")\n",
    "context_aware_copy_editor_agent = prelude | copy_editor_agent\n",
    "copy_editing_node = functools.partial(\n",
    "    agent_node, agent=context_aware_copy_editor_agent, name=\"CopyEditor\"\n",
    ")\n",
    "\n",
    "authoring_supervisor_agent = create_team_supervisor(\n",
    "    authoring_llm,\n",
    "    (\"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers: {team_members}. You should always verify the technical\"\n",
    "    \" contents after any edits are made. \"\n",
    "    \"Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When each team is finished,\"\n",
    "    \" you must respond with FINISH.\"),\n",
    "    [\"DocWriter\", \"NoteTaker\", \"CopyEditor\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUiNMpJBGXN0"
   },
   "source": [
    "### Document Writing Team LangGraph Construction\n",
    "\n",
    "This part is almost exactly the same (with a few extra nodes) as our Research Team LangGraph construction - so we'll leave it as one block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6n8A1ytxVTv"
   },
   "outputs": [],
   "source": [
    "authoring_graph = StateGraph(DocWritingState)\n",
    "authoring_graph.add_node(\"DocWriter\", doc_writing_node)\n",
    "authoring_graph.add_node(\"NoteTaker\", note_taking_node)\n",
    "authoring_graph.add_node(\"CopyEditor\", copy_editing_node)\n",
    "authoring_graph.add_node(\"AuthoringSupervisor\", authoring_supervisor_agent)\n",
    "\n",
    "authoring_graph.add_edge(\"DocWriter\", \"AuthoringSupervisor\")\n",
    "authoring_graph.add_edge(\"NoteTaker\", \"AuthoringSupervisor\")\n",
    "authoring_graph.add_edge(\"CopyEditor\", \"AuthoringSupervisor\")\n",
    "\n",
    "authoring_graph.add_conditional_edges(\n",
    "    \"AuthoringSupervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\n",
    "        \"DocWriter\": \"DocWriter\",\n",
    "        \"NoteTaker\": \"NoteTaker\",\n",
    "        \"CopyEditor\" : \"CopyEditor\",\n",
    "        \"FINISH\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "authoring_graph.set_entry_point(\"AuthoringSupervisor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_authoring_graph = authoring_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx-EKGkHKUBO"
   },
   "source": [
    "#### Display Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_authoring_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB_rOw1hGpwd"
   },
   "source": [
    "Just as before - we'll need to create an \"interface\" between the level above, and our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-RbbCKoG_nt"
   },
   "outputs": [],
   "source": [
    "def enter_authoring_chain(message: str, members: List[str]):\n",
    "    results = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"team_members\": \", \".join(members),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "authoring_chain = (\n",
    "    functools.partial(enter_authoring_chain, members=authoring_graph.nodes)\n",
    "    | compiled_authoring_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgyhpTrRNgQd"
   },
   "source": [
    "Now we can test this out!\n",
    "\n",
    "> NOTE: It is possible you may see an error here - rerun the cell to clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWUxv4XDx3kg",
    "outputId": "62ee7d3d-31ba-4348-b852-7fd96f6875ff"
   },
   "outputs": [],
   "source": [
    "for s in authoring_chain.stream(\n",
    "    \"What are the most common use-cases in this data. What are the most common domains?\",\n",
    "    {\"recursion_limit\": 100},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpW2R9SUHGUq"
   },
   "source": [
    "## Task 5: Meta-Supervisor and Full Graph\n",
    "\n",
    "Finally, now that we have our two LangGraph agents (some of which are already multi-agent), we can build a supervisor that sits above all of them!\n",
    "\n",
    "The final process, surprisingly, is quite straight forward!\n",
    "\n",
    "Let's jump in!\n",
    "\n",
    "First off - we'll need to create our supervisor agent node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkpxeUf9ygKp"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "super_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "super_supervisor_agent = create_team_supervisor(\n",
    "    super_llm,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following teams: {team_members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When all workers are finished,\"\n",
    "    \" you must respond with FINISH.\",\n",
    "    [\"Research team\", \"Response team\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUvOh_xWIKig"
   },
   "source": [
    "We'll also create our new state - as well as some methods to help us navigate the new state and the subgraphs.\n",
    "\n",
    "> NOTE: We only pass the most recent message from the parent graph to the subgraph, and we only extract the most recent message from the subgraph to include in the state of the parent graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7HJ8MF0yh_i"
   },
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    next: str\n",
    "\n",
    "def get_last_message(state: State) -> str:\n",
    "    return state[\"messages\"][-1].content\n",
    "\n",
    "def join_graph(response: dict):\n",
    "    return {\"messages\": [response[\"messages\"][-1]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5RHao1sIanG"
   },
   "source": [
    "Next, we'll create our base graph.\n",
    "\n",
    "Notice how each node we're adding is *AN ENTIRE LANGGRAPH AGENT* (wrapped into an LCEL chain with our helper functions above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfCWABCMIaFy"
   },
   "outputs": [],
   "source": [
    "super_graph = StateGraph(State)\n",
    "\n",
    "super_graph.add_node(\"Research team\", get_last_message | research_chain | join_graph)\n",
    "super_graph.add_node(\"Response team\", get_last_message | authoring_chain | join_graph)\n",
    "super_graph.add_node(\"SuperSupervisor\", super_supervisor_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpwpUXMtI62E"
   },
   "source": [
    "Next, we'll create our edges!\n",
    "\n",
    "This process is completely idenctical to what we've seen before - just addressing the LangGraph subgraph nodes instead of individual nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLtjRuUYI-fx"
   },
   "outputs": [],
   "source": [
    "super_graph.add_edge(\"Research team\", \"SuperSupervisor\")\n",
    "super_graph.add_edge(\"Response team\", \"SuperSupervisor\")\n",
    "super_graph.add_conditional_edges(\n",
    "    \"SuperSupervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\n",
    "        \"Response team\": \"Response team\",\n",
    "        \"Research team\": \"Research team\",\n",
    "        \"FINISH\": END,\n",
    "    },\n",
    ")\n",
    "super_graph.set_entry_point(\"SuperSupervisor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_super_graph = super_graph.compile()\n",
    "compiled_super_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1KMfFqgJKw8"
   },
   "source": [
    "That's it!\n",
    "\n",
    "Now we can finally use our full agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3M6wUDR-yk8s",
    "outputId": "056fe89e-5a81-4852-f0cb-35367da8cef0"
   },
   "outputs": [],
   "source": [
    "WORKING_DIRECTORY = Path(create_random_subdirectory())\n",
    "\n",
    "for s in compiled_super_graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Write a report on the rise of context engineering in the LLM Space in 2025, and how it's impacting how people are using AI.\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    {\"recursion_limit\": 30},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuZAvSlJJpPP"
   },
   "source": [
    "## SAMPLE POST!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOEMCrXTJaxW"
   },
   "source": [
    "### Report on the Rise of Context Engineering in the LLM Space in 2025\n",
    "\n",
    "#### Introduction\n",
    "As we move further into 2025, the landscape of artificial intelligence continues to evolve, particularly in the domain of large language models (LLMs). One significant trend emerging is \"context engineering,\" which has begun to outpace the earlier focus on prompt engineering. This report explores how context engineering is reshaping the dynamics of AI usage, enhancing the capabilities of LLMs to perform complex tasks, and ultimately influencing the way users interact with AI technologies.\n",
    "\n",
    "#### The Emergence of Context Engineering\n",
    "In previous years, prompt engineering involved crafting inputs to guide LLMs to generate desired outputs. However, by 2025, the limitations of traditional prompt engineering have become evident, especially as applications require more dynamic interactions and multi-turn dialogues. Context engineering has emerged as a new paradigm, prioritizing the construction of strategies and systems that enable LLMs to access and utilize extensive contextual information effectively.\n",
    "\n",
    "Modern LLMs, such as GPT-4 Turbo and Claude 3, now boast context windows that can handle up to 1 million tokens. This substantial increase allows these models to grasp more extensive content, such as books or lengthy documents, fundamentally changing what they can achieve in real-world applications.\n",
    "\n",
    "#### Key Developments in Context Engineering\n",
    "1. **Dynamic Knowledge Management**: The shift towards context engineering facilitates more sophisticated knowledge management within AI systems. These systems now use persistent memory and adaptive reasoning, enabling them to maintain context across interactions. For example, agent-based LLMs can now engage in workflows requiring multiple exchanges, which is vital for complex applications requiring follow-ups and extended dialogues.\n",
    "\n",
    "2. **Improved Context Lengths**: Advances in transformer architectures have led to the possibility of extending native context lengths dramatically. As a result, LLMs can process larger datasets directly, enhancing their capacity to deliver accurate and relevant outputs without extensive re-prompting.\n",
    "\n",
    "3. **Context Compression and Retrieval**: Techniques such as context compression, where information is stored externally or synthesized to maximize relevance, are gaining traction. Methods identified in recent studies highlight the importance of integrating efficient retrieval systems and summarization techniques to make the most of the available context.\n",
    "\n",
    "4. **The Rise of Multi-Agent Systems**: In 2025, LLM applications are increasingly shifting towards multi-agent architectures, where different AI agents work collaboratively, leveraging various tools and pieces of information to improve output accuracy and reduce operational inefficiencies.\n",
    "\n",
    "#### Impact on AI Usage\n",
    "The implications of context engineering are far-reaching and transformative for users and organizations:\n",
    "\n",
    "- **Enhanced Accuracy**: Enterprises adopting context engineering report accuracy rates of 90-95%, a significant improvement over the 65-75% typically associated with earlier methods. This rise in performance is generating more trust in AI systems across various sectors.\n",
    "\n",
    "- **Faster Deployment and Cost Efficiency**: Organizations employing structured context engineering have demonstrated a 3x increase in the speed of bringing AI solutions to production and a 40% reduction in operational costs compared to traditional prompt-based methodologies.\n",
    "\n",
    "- **Adaptability and Scalability**: Context-engineered systems are inherently more adaptable, allowing businesses to scale AI applications rapidly in response to evolving needs without overhauling the underlying architecture.\n",
    "\n",
    "- **Augmentation of Human Intelligence**: As context engineering allows AI systems to interact in more meaningful ways, they are increasingly positioned as partners in decision-making processes, rather than mere tools. This evolution enhances the collaborative potential between humans and AI.\n",
    "\n",
    "#### Conclusion\n",
    "The rise of context engineering marks a critical turning point in the development and application of LLMs. As AI systems become increasingly capable of managing and utilizing complex contextual information, they are set to enhance user experiences, improve efficiency, and drive new innovations across industries. Organizations that embrace this trend will likely lead the charge into a future where AI is not just a tool for automation but a collaborative partner in problem-solving and creativity."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
