{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253a504f",
   "metadata": {},
   "source": [
    "# Online PDF RAG System - LangGraph\n",
    "\n",
    "Today we'll build a RAG (Retrieval Augmented Generation) system that downloads a PDF from an online URL and processes it for question-answering!\n",
    "\n",
    "Instead of using hardcoded local PDFs, this system will:\n",
    "1. Download a PDF from arXiv URL: https://arxiv.org/pdf/2509.22613\n",
    "2. Save it to our data folder\n",
    "3. Process it through the same RAG pipeline as the original code\n",
    "\n",
    "This approach allows us to work with any online PDF dynamically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb5973",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Since we'll be relying on OpenAI's suite of models to power our agents today, we'll want to provide our OpenAI API Key.\n",
    "\n",
    "We're also adding requests library to download PDFs from online URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec012d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fafdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38289df1",
   "metadata": {},
   "source": [
    "## Task 1: Online PDF RAG System\n",
    "\n",
    "Now let's create a RAG system that dynamically downloads and processes PDFs from online URLs!\n",
    "\n",
    "> NOTE: This approach allows us to work with any PDF available online, making our system much more flexible than hardcoded local files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669fe90c",
   "metadata": {},
   "source": [
    "## PDF Download and Setup\n",
    "\n",
    "First, let's download the PDF from arXiv and save it to our data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4f635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PDF from https://arxiv.org/pdf/2509.22613...\n",
      "‚úÖ PDF downloaded and saved to: data/arxiv_paper.pdf\n",
      "üìÑ File size: 4741.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Download PDF from arXiv URL\n",
    "pdf_url = \"https://arxiv.org/pdf/2509.22613\"\n",
    "data_folder = Path(\"data\")\n",
    "pdf_filename = \"arxiv_paper.pdf\"\n",
    "pdf_path = data_folder / pdf_filename\n",
    "\n",
    "# Create data folder if it doesn't exist\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Download and save the PDF\n",
    "print(f\"Downloading PDF from {pdf_url}...\")\n",
    "response = requests.get(pdf_url)\n",
    "response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "\n",
    "with open(pdf_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(f\"‚úÖ PDF downloaded and saved to: {pdf_path}\")\n",
    "print(f\"üìÑ File size: {pdf_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcada4",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "The 'R' in 'RAG' - now let's process our downloaded PDF!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b946893",
   "metadata": {},
   "source": [
    "#### Data Collection and Processing\n",
    "\n",
    "Now let's load our downloaded PDF document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a740b6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 23 pages from the PDF\n",
      "üìÑ First page preview: Preprint as an Arxiv Paper\n",
      "BENEFITS\n",
      "AND\n",
      "PITFALLS\n",
      "OF\n",
      "REINFORCEMENT\n",
      "LEARNING FOR LANGUAGE MODEL PLANNING:\n",
      "A THEORETICAL PERSPECTIVE\n",
      "Siwei Wang1‚Ä†, Yifei Shen1‚Ä†, Haoran Sun2‚Ä†, Shi Feng3‚Ä†, Shang-Hua Teng4,...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Load the downloaded PDF\n",
    "pdf_loader = PyMuPDFLoader(str(pdf_path))\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} pages from the PDF\")\n",
    "print(f\"üìÑ First page preview: {documents[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b105d",
   "metadata": {},
   "source": [
    "Now we can chunk it down to size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef6ae01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 41 text chunks from the PDF\n",
      "üìÑ First chunk preview: Preprint as an Arxiv Paper\n",
      "BENEFITS\n",
      "AND\n",
      "PITFALLS\n",
      "OF\n",
      "REINFORCEMENT\n",
      "LEARNING FOR LANGUAGE MODEL PLANNING:\n",
      "A THEORETICAL PERSPECTIVE\n",
      "Siwei Wang1‚Ä†, Yifei Shen1‚Ä†, Haoran Sun2‚Ä†, Shi Feng3‚Ä†, Shang-Hua Teng4,...\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = tiktoken_len,\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Created {len(document_chunks)} text chunks from the PDF\")\n",
    "print(f\"üìÑ First chunk preview: {document_chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec8d0e",
   "metadata": {},
   "source": [
    "### üìä Creating Embeddings & Vector Store\n",
    "\n",
    "Now we'll create embeddings for our document chunks and store them in a vector database for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c0652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created successfully!\n",
      "üìÅ Collection: pdf_documents_419c8480\n",
      "‚è±Ô∏è Processing time: 2.61 seconds\n",
      "üî¢ Total documents indexed: 41\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "# Initialize Qdrant client (in-memory)\n",
    "qdrant_client = QdrantClient(':memory:')\n",
    "\n",
    "# Create collection\n",
    "collection_name = f\"pdf_documents_{uuid.uuid4().hex[:8]}\"\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client, \n",
    "    collection_name=collection_name, \n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Add documents to vector store\n",
    "vector_store.add_documents(documents=document_chunks)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"‚úÖ Vector store created successfully!\")\n",
    "print(f\"üìÅ Collection: {collection_name}\")\n",
    "print(f\"‚è±Ô∏è Processing time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"üî¢ Total documents indexed: {len(document_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdff72",
   "metadata": {},
   "source": [
    "### ü§ñ Setting up the RAG System with LangGraph\n",
    "\n",
    "Now we'll create our retrieval and generation functions using LangGraph for a multi-agent approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4c6f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG system initialized successfully!\n",
      "üîç Retriever configured to return top 5 relevant documents\n",
      "üí¨ Language model: gpt-4o-mini\n",
      "üåü Ready to answer questions about the PDF content!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    documents: list\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    \"\"\"Retrieve documents relevant to the question\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\"Generate an answer based on the retrieved documents\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Create context from documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful AI assistant. Answer the question based on the provided context.\n",
    "        If you can't find the answer in the context, say so. Be concise and accurate.\n",
    "        \n",
    "        Context:\n",
    "        {context}\"\"\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    # Generate response\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the graph\n",
    "rag_app = workflow.compile()\n",
    "\n",
    "print(\"‚úÖ RAG system initialized successfully!\")\n",
    "print(\"üîç Retriever configured to return top 5 relevant documents\")\n",
    "print(\"üí¨ Language model: gpt-4o-mini\")\n",
    "print(\"üåü Ready to answer questions about the PDF content!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b2a8a",
   "metadata": {},
   "source": [
    "### üß™ Testing the RAG System\n",
    "\n",
    "Let's test our RAG system with some sample questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdfb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str):\n",
    "    \"\"\"Helper function to ask questions to our RAG system\"\"\"\n",
    "    print(f\"üîç Question: {question}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    result = rag_app.invoke({\"question\": question})\n",
    "    \n",
    "    print(f\"üìö Retrieved {len(result['documents'])} relevant documents\")\n",
    "    print(f\"üí° Answer: {result['answer']}\")\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with sample questions\n",
    "sample_questions = [\n",
    "    \"What is this document about?\",\n",
    "    \"What are the main findings or conclusions?\",\n",
    "    \"Who are the authors of this paper?\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Testing RAG System with Sample Questions\\n\")\n",
    "\n",
    "for question in sample_questions:\n",
    "    try:\n",
    "        ask_question(question)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with question '{question}': {str(e)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9200f5",
   "metadata": {},
   "source": [
    "### üéØ Interactive Q&A\n",
    "\n",
    "Now you can ask your own questions about the PDF content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e6f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask your own questions here!\n",
    "# Example usage:\n",
    "# ask_question(\"Your question here\")\n",
    "\n",
    "# Uncomment and modify the line below to ask your own question:\n",
    "# ask_question(\"What specific methodology was used in this research?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
