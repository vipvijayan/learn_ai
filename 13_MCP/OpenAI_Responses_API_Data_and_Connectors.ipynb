{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4ab430",
   "metadata": {},
   "source": [
    "# OpenAI Responses API - Data and Connectors Demo\n",
    "\n",
    "This notebook demonstrates the powerful **File Search** capability of the OpenAI Responses API, which allows models to retrieve information from your uploaded documents through semantic and keyword search.\n",
    "\n",
    "The File Search tool offers several key advantages:\n",
    "- **Semantic search** - Find relevant information by meaning, not just keywords\n",
    "- **Vector store integration** - Scalable knowledge base management\n",
    "- **Automatic citations** - Get references to source documents in responses\n",
    "- **Hosted solution** - No need to implement search infrastructure yourself\n",
    "- **Retrieval customization** - Control search results, filtering, and metadata\n",
    "- **Multiple file formats** - Support for PDFs, docs, code files, and more\n",
    "\n",
    "This notebook walks through setting up vector stores, uploading documents, and using file search with the Responses API to create AI applications that can reason over your data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c97ec0",
   "metadata": {},
   "source": [
    "### Setup and Authentication\n",
    "\n",
    "First, we need to set up our OpenAI API credentials. We'll use `getpass` to securely input the API key without exposing it in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f21995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca7137",
   "metadata": {},
   "source": [
    "Now we'll initialize the OpenAI client and import the additional libraries we'll need for file handling and requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba4bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a282edd",
   "metadata": {},
   "source": [
    "### Setting Up File Search: Vector Stores and File Upload\n",
    "\n",
    "Before we can use file search with the Responses API, we need to create a knowledge base. This involves three key steps:\n",
    "\n",
    "1. **Upload files** to the OpenAI File API\n",
    "2. **Create a vector store** to organize our knowledge base\n",
    "3. **Add files to the vector store** for semantic search\n",
    "\n",
    "Let's start by creating a helper function that can handle both local files and URLs, similar to the example in the OpenAI documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698db8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(client, file_path):\n",
    "    \"\"\"\n",
    "    Upload a file to OpenAI's File API.\n",
    "    Supports both local file paths and URLs.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        file_path: Path to local file or URL to download from\n",
    "        \n",
    "    Returns:\n",
    "        str: File ID from OpenAI\n",
    "    \"\"\"\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        # Download the file content from the URL\n",
    "        print(f\"Downloading file from URL: {file_path}\")\n",
    "        response = requests.get(file_path)\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        file_tuple = (file_name, file_content)\n",
    "        result = client.files.create(\n",
    "            file=file_tuple,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        print(f\"Uploading local file: {file_path}\")\n",
    "        with open(file_path, \"rb\") as file_content:\n",
    "            result = client.files.create(\n",
    "                file=file_content,\n",
    "                purpose=\"assistants\"\n",
    "            )\n",
    "    \n",
    "    print(f\"File uploaded successfully. File ID: {result.id}\")\n",
    "    return result.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6699f21",
   "metadata": {},
   "source": [
    "Now let's upload your local PDF file about embeddings from the data folder. This will serve as our knowledge base for testing file search functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a2de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading local file: /home/chris/Code/AI Makerspace/Events/OpenAI Responses API/data/Embedding-Based.pdf\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/chris/Code/AI Makerspace/Events/OpenAI Responses API/data/Embedding-Based.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Upload your local PDF file from the data folder\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m file_id = \u001b[43mcreate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/chris/Code/AI Makerspace/Events/OpenAI Responses API/data/Embedding-Based.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mcreate_file\u001b[39m\u001b[34m(client, file_path)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Handle local file path\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUploading local file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_content:\n\u001b[32m     28\u001b[39m         result = client.files.create(\n\u001b[32m     29\u001b[39m             file=file_content,\n\u001b[32m     30\u001b[39m             purpose=\u001b[33m\"\u001b[39m\u001b[33massistants\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile uploaded successfully. File ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/projects/AI/AIMakerSpace/code/learn_ai_0/activate/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/chris/Code/AI Makerspace/Events/OpenAI Responses API/data/Embedding-Based.pdf'"
     ]
    }
   ],
   "source": [
    "# Upload your local PDF file from the data folder\n",
    "file_id = create_file(client, \"/home/chris/Code/AI Makerspace/Events/OpenAI Responses API/data/Embedding-Based.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ecc16",
   "metadata": {},
   "source": [
    "Next, we need to create a **vector store**. Think of this as a container that organizes your uploaded files for semantic search. The vector store will automatically process your documents and create embeddings for efficient retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17983ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully!\n",
      "Vector Store ID: vs_68c1aea8ca8081918962708d7420ffde\n",
      "Name: AI_Makerspace_Knowledge_Base\n",
      "Status: completed\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store for our knowledge base\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"AI_Makerspace_Knowledge_Base\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created successfully!\")\n",
    "print(f\"Vector Store ID: {vector_store.id}\")\n",
    "print(f\"Name: {vector_store.name}\")\n",
    "print(f\"Status: {vector_store.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5e7f7",
   "metadata": {},
   "source": [
    "Now we'll add our uploaded file to the vector store. This step tells OpenAI to process the document and make it available for semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File added to vector store successfully!\n",
      "File ID: file-WrMJNKgKCN71PzTs2BQrE4\n",
      "Status: in_progress\n",
      "Created at: 1757523628\n"
     ]
    }
   ],
   "source": [
    "# Add the uploaded file to our vector store\n",
    "result = client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file_id\n",
    ")\n",
    "\n",
    "print(f\"File added to vector store successfully!\")\n",
    "print(f\"File ID: {result.id}\")\n",
    "print(f\"Status: {result.status}\")\n",
    "print(f\"Created at: {result.created_at}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f55aa",
   "metadata": {},
   "source": [
    "We need to wait for the file processing to complete before we can use it for search. Let's check the status and wait until it's ready. The file needs to be in `completed` status for search to work properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File statuses: ['completed']\n",
      "âœ… All files processed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_file_processing(client, vector_store_id, max_wait_time=300):\n",
    "    \"\"\"\n",
    "    Wait for all files in a vector store to be processed.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        vector_store_id: ID of the vector store to check\n",
    "        max_wait_time: Maximum time to wait in seconds (default: 5 minutes)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all files are completed, False if timeout\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_wait_time:\n",
    "        # Check the status of files in the vector store\n",
    "        files = client.vector_stores.files.list(vector_store_id=vector_store_id)\n",
    "        \n",
    "        statuses = [file.status for file in files.data]\n",
    "        print(f\"File statuses: {statuses}\")\n",
    "        \n",
    "        if all(status == \"completed\" for status in statuses):\n",
    "            print(\"âœ… All files processed successfully!\")\n",
    "            return True\n",
    "        elif any(status == \"failed\" for status in statuses):\n",
    "            print(\"âŒ One or more files failed to process\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"â³ Still processing... waiting 10 seconds\")\n",
    "            time.sleep(10)\n",
    "    \n",
    "    print(\"â° Timeout waiting for file processing\")\n",
    "    return False\n",
    "\n",
    "# Wait for our file to be processed\n",
    "wait_for_file_processing(client, vector_store.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfacb6",
   "metadata": {},
   "source": [
    "### Basic File Search with the Responses API\n",
    "\n",
    "Now that our knowledge base is set up, we can use the **file search** tool with the Responses API! This is where the magic happens - the model can automatically search through your documents and provide answers with citations.\n",
    "\n",
    "Key features of file search:\n",
    "- **Automatic tool calling** - The model decides when to search your files\n",
    "- **Semantic understanding** - Finds relevant information by meaning, not just keywords  \n",
    "- **Source citations** - Get references to specific documents in the response\n",
    "- **Multiple output types** - Both search calls and final messages with citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d2c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Response from file search:\n",
      "==================================================\n",
      "Dense (singleâ€‘vector) embeddings have a fundamental capacity limit: because their representation power is bounded by the embedding dimension, they cannot realize all possible topâ€‘k relevance combinations; for any fixed dimension d there exist queries/relevance patterns they will fail to return, no matter how theyâ€™re trained . This limitation can be formalized via signâ€‘rank: some relevance matrices require higher dimensionality than the model has, so dense embeddings cannot capture them exactly . As tasks demand more combinations (e.g., instructionâ€‘based retrieval), the dimensionality needed grows rapidly and becomes impractical at scale .\n"
     ]
    }
   ],
   "source": [
    "# Use file search with the Responses API\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What is the main problem with dense vector embeddings?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"ðŸ” Response from file search:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151b393",
   "metadata": {},
   "source": [
    "Let's examine the full response structure to understand what file search returns. The response contains multiple output items including the search call details and the final message with citations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31037ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Full Response Analysis:\n",
      "==================================================\n",
      "\n",
      "ðŸ“‹ Output Item 1:\n",
      "   Type: reasoning\n",
      "   ID: rs_68c1b00540f081a081e58d4e59bff7ab047aaa5f6e571614\n",
      "\n",
      "ðŸ“‹ Output Item 2:\n",
      "   Type: file_search_call\n",
      "   ID: fs_68c1b008dea081a0a679634f7078f796047aaa5f6e571614\n",
      "   Status: completed\n",
      "   Queries: ['What is the main problem with dense vector embeddings?', 'limitations of dense vector embeddings main problem', 'dense embeddings problem opacity interpretability sparsity', 'dense vs sparse embeddings advantages disadvantages', 'What is the problem with dense vector representations in NLP?']\n",
      "\n",
      "ðŸ“‹ Output Item 3:\n",
      "   Type: reasoning\n",
      "   ID: rs_68c1b00b202881a0ace83f23d5e7eb18047aaa5f6e571614\n",
      "\n",
      "ðŸ“‹ Output Item 4:\n",
      "   Type: message\n",
      "   ID: msg_68c1b01ec16881a0b1554faa33adee5c047aaa5f6e571614\n",
      "   Role: assistant\n",
      "   Content items: 1\n",
      "   ðŸ“š Found 3 citations\n",
      "      Citation 1: Embedding-Based.pdf\n",
      "      Citation 2: Embedding-Based.pdf\n",
      "      Citation 3: Embedding-Based.pdf\n"
     ]
    }
   ],
   "source": [
    "# Examine the full response structure\n",
    "print(\"ðŸ“Š Full Response Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, output_item in enumerate(response.output):\n",
    "    print(f\"\\nðŸ“‹ Output Item {i + 1}:\")\n",
    "    print(f\"   Type: {output_item.type}\")\n",
    "    print(f\"   ID: {output_item.id}\")\n",
    "    \n",
    "    if output_item.type == \"file_search_call\":\n",
    "        print(f\"   Status: {output_item.status}\")\n",
    "        print(f\"   Queries: {output_item.queries}\")\n",
    "        \n",
    "    elif output_item.type == \"message\":\n",
    "        print(f\"   Role: {output_item.role}\")\n",
    "        print(f\"   Content items: {len(output_item.content)}\")\n",
    "        \n",
    "        # Check for citations in the message content\n",
    "        for content_item in output_item.content:\n",
    "            if hasattr(content_item, 'annotations') and content_item.annotations:\n",
    "                print(f\"   ðŸ“š Found {len(content_item.annotations)} citations\")\n",
    "                for j, annotation in enumerate(content_item.annotations):\n",
    "                    if annotation.type == \"file_citation\":\n",
    "                        print(f\"      Citation {j + 1}: {annotation.filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4765a",
   "metadata": {},
   "source": [
    "### Including Search Results in the Response\n",
    "\n",
    "By default, the file search call doesn't return the actual search results - only the final answer with citations. However, you can include the search results using the `include` parameter. This is useful for understanding what information was retrieved and how the model used it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a5ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Response with search results:\n",
      "==================================================\n",
      "Here are the main limitations practitioners run into with dense, singleâ€‘vector embeddings:\n",
      "\n",
      "- Finite representational capacity tied to dimension d: For any fixed d, there exist relevance patterns (topâ€‘k sets) that a single-vector embedder simply cannot realizeâ€”no amount of training data fixes this. This follows from signâ€‘rank bounds on the relevance (qrel) matrix and yields an intrinsic ceiling on what a dâ€‘dimensional model can retrieve exactly .\n",
      "\n",
      "- Breakdowns as combinations grow: As tasks demand returning many different combinations of relevant documents (e.g., instructionâ€‘following or reasoning queries that connect previously unrelated items), dense retrievers hit these capacity limits. Empirically, even stateâ€‘ofâ€‘theâ€‘art models struggle on LIMIT, a simple dataset constructed to stress such combinations, with recall remaining low despite trivial content  . The â€œdenseâ€ qrel pattern that maximizes combinations is especially damaging across models .\n",
      "\n",
      "- You canâ€™t just scale a bit to escape it: Performance improves with higher embedding dimensions, but representing all necessary combinations would require infeasibly large d for realistic corpora. This tradeâ€‘off is explicit both in theory and in experiments showing a critical point where increasing corpus/combination complexity outpaces the modelâ€™s dimensional capacity  .\n",
      "\n",
      "- Singleâ€‘vector bottleneck versus more expressive retrievers: Encoding an entire query or document into one vector limits expressiveness. Multiâ€‘vector lateâ€‘interaction models and sparse lexical methods (effectively very highâ€‘dimensional) avoid part of this bottleneck and fare much better on LIMIT, though with their own tradeâ€‘offs. Crossâ€‘encoders donâ€™t share the same dimensional limits but are too expensive for firstâ€‘stage retrieval at scale  .\n",
      "\n",
      "- Degradation at large index sizes (â€œcurseâ€ for lowâ€‘dimensional dense IR): Prior work shows smallerâ€‘dimension dense models produce more false positives as the corpus grows; dense lowâ€‘dimensional IR degrades with large indexes, reinforcing the practical impact of limited dimensionality .\n",
      "\n",
      "- Benchmarks can mask the problem: Common IR benchmarks cover a narrow slice of possible queries and can be overfit, hiding these limits. On LIMIT, performance is poorly correlated with BEIR, underscoring that standard scores may not reveal true capacity constraints  .\n",
      "\n",
      "- Nonâ€‘metric behavior of common similarities: Cosine similarity is not a metric, so triangleâ€‘inequalityâ€“based intuitions and some theoretical guarantees or pruning strategies donâ€™t apply as they might in metric spaces, complicating analysis and some indexing optimizations .\n",
      "\n",
      "In short, dense singleâ€‘vector embeddings are powerful but fundamentally capacityâ€‘limited by embedding dimension; as retrieval tasks require representing more diverse and combinatorial notions of relevance, these limits become unavoidable without switching to more expressive architectures or adding expensive reranking stages  .\n"
     ]
    }
   ],
   "source": [
    "# Include search results in the response\n",
    "response_with_results = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What are the key limitations of dense vector embeddings?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }],\n",
    "    include=[\"file_search_call.results\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸ” Response with search results:\")\n",
    "print(\"=\" * 50)\n",
    "print(response_with_results.output_text)\n",
    "\n",
    "# Examine the search results\n",
    "for output_item in response_with_results.output:\n",
    "    if output_item.type == \"file_search_call\" and hasattr(output_item, 'search_results'):\n",
    "        if output_item.search_results:\n",
    "            print(f\"\\nðŸ“„ Search Results Found:\")\n",
    "            print(f\"Number of results: {len(output_item.search_results)}\")\n",
    "            \n",
    "            for i, result in enumerate(output_item.search_results):\n",
    "                print(f\"\\nResult {i + 1}:\")\n",
    "                print(f\"  Score: {result.score}\")\n",
    "                print(f\"  Content preview: {result.content[:200]}...\")\n",
    "        else:\n",
    "            print(\"\\nðŸ“„ No search results returned (search_results is None)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395be437",
   "metadata": {},
   "source": [
    "### Retrieval Customization\n",
    "\n",
    "The file search tool offers several customization options to optimize performance and results:\n",
    "\n",
    "1. **Limiting results** - Control the number of search results to reduce tokens and latency\n",
    "2. **Metadata filtering** - Filter search results based on file attributes\n",
    "3. **Search quality vs. performance** - Balance between comprehensive results and response speed\n",
    "\n",
    "Let's explore these customization options:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df98e54",
   "metadata": {},
   "source": [
    "#### Limiting the Number of Results\n",
    "\n",
    "By default, file search may retrieve many results. You can limit this using `max_num_results` to reduce token usage and improve response time, though this may impact answer quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e82112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Limited Results Response:\n",
      "==================================================\n",
      "The core problem is limited expressivity: a single fixedâ€‘dimensional dense vector canâ€™t encode all the combinations of documentâ€“query relevance you may need. For any chosen dimension, there exist topâ€‘k relevance patterns that cannot be realizedâ€”no matter how you train the modelâ€”so as corpora and tasks grow, dense embeddings hit a hard capacity limit unless you dramatically increase dimensionality . In fact, theoretical bounds show the required dimensionality can be much larger than whatâ€™s practical for real IR problems, which leads to recall errors and false positives at scale .\n"
     ]
    }
   ],
   "source": [
    "# Limit the number of search results\n",
    "response_limited = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"What is the main problem with dense vector embeddings?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id],\n",
    "        \"max_num_results\": 2  # Limit to 2 results for faster, more focused responses\n",
    "    }],\n",
    "    include=[\"file_search_call.results\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ¯ Limited Results Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(response_limited.output_text)\n",
    "\n",
    "# Show how many results were actually used\n",
    "for output_item in response_limited.output:\n",
    "    if output_item.type == \"file_search_call\":\n",
    "        if hasattr(output_item, 'search_results') and output_item.search_results:\n",
    "            print(f\"\\nðŸ“Š Used {len(output_item.search_results)} search results (limited to 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae0f6a",
   "metadata": {},
   "source": [
    "#### Practical Example: Adding Local Files to Your Knowledge Base\n",
    "\n",
    "Let's demonstrate how to add local files from your project to create a more comprehensive knowledge base. We'll check if there are any PDF files in the data directory and add them to our vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1bd3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Found 2 PDF files in ./data:\n",
      "   - Embedding-Based.pdf\n",
      "   - Harnessing Embeddings.pdf\n",
      "\n",
      "ðŸ“¤ Uploading Embedding-Based.pdf...\n",
      "Uploading local file: ./data/Embedding-Based.pdf\n",
      "File uploaded successfully. File ID: file-EunX48ia7QfQNNhXppgJgK\n",
      "âœ… Added to vector store successfully!\n",
      "\n",
      "â³ Waiting for new files to be processed...\n",
      "File statuses: ['in_progress', 'completed']\n",
      "â³ Still processing... waiting 10 seconds\n",
      "File statuses: ['completed', 'completed']\n",
      "âœ… All files processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Check for local PDF files in the data directory\n",
    "data_dir = \"./data\"\n",
    "pdf_files = []\n",
    "\n",
    "if os.path.exists(data_dir):\n",
    "    pdf_files = glob.glob(os.path.join(data_dir, \"*.pdf\"))\n",
    "    print(f\"ðŸ“ Found {len(pdf_files)} PDF files in {data_dir}:\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"   - {os.path.basename(pdf_file)}\")\n",
    "else:\n",
    "    print(f\"ðŸ“ Data directory {data_dir} not found\")\n",
    "\n",
    "# Upload and add local files to the vector store\n",
    "local_file_ids = []\n",
    "for pdf_file in pdf_files[:1]:  # Let's add just the first PDF to avoid too much processing time\n",
    "    try:\n",
    "        print(f\"\\nðŸ“¤ Uploading {os.path.basename(pdf_file)}...\")\n",
    "        local_file_id = create_file(client, pdf_file)\n",
    "        local_file_ids.append(local_file_id)\n",
    "        \n",
    "        # Add to vector store\n",
    "        result = client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store.id,\n",
    "            file_id=local_file_id\n",
    "        )\n",
    "        print(f\"âœ… Added to vector store successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {pdf_file}: {e}\")\n",
    "\n",
    "if local_file_ids:\n",
    "    print(f\"\\nâ³ Waiting for new files to be processed...\")\n",
    "    wait_for_file_processing(client, vector_store.id)\n",
    "else:\n",
    "    print(\"\\nðŸ’¡ No local files were added. You can place PDF files in the ./data directory to test with your own documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdd3ac",
   "metadata": {},
   "source": [
    "### Multi-Document Search and Comparison\n",
    "\n",
    "Now that we have multiple documents in our knowledge base, we can ask questions that require searching across different sources. This demonstrates the power of semantic search for complex queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f992e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Complex Multi-Document Query:\n",
      "============================================================\n",
      "Below is a concise map of the main AI R&D approaches and methodologies discussed in the documents, with their benefits and tradeâ€‘offs.\n",
      "\n",
      "1) Theoretical analysis of model limits\n",
      "- What it is: Use tools from communication complexity and signâ€‘rank to prove lower bounds on what singleâ€‘vector (dense) embedding models can represent in topâ€‘k retrieval; relates but contrasts with geometric intuitions like orderâ€‘k Voronoi regions. Also clarifies why triangleâ€‘inequality arguments donâ€™t apply when using cosine similarity (nonâ€‘metric)  .\n",
      "- Benefits:\n",
      "  - Gives rigorous, architectureâ€‘agnostic limits for singleâ€‘vector embeddings across modalities: for a fixed embedding dimension d, some topâ€‘k combinations simply cannot be returned by any query  .\n",
      "  - Guides system design by explaining why scaling tasks that require many combinatorial combinations will eventually hit representation ceilings .\n",
      "- Tradeâ€‘offs:\n",
      "  - Results target the dominant singleâ€‘vector paradigm; extensions to other architectures (e.g., multiâ€‘vector) are left for future work .\n",
      "\n",
      "2) Empirical stressâ€‘testing and dataset construction\n",
      "- What it is: â€œBestâ€‘caseâ€ empirical tests that directly optimize vectors on the test set (â€œfree embeddingsâ€), plus construction of the LIMIT dataset to stress topâ€‘k combination capacity with simple queries; ablations over qrel graph density show why dense combination patterns are uniquely hard  .\n",
      "- Benefits:\n",
      "  - Confirms the theoretical bounds in practice and isolates dimensionality as the critical bottleneck, even on trivial language tasks; models struggle markedly as combinations grow, and performance rises with larger dimensions .\n",
      "  - Shows the difficulty isnâ€™t mere domain shift: fineâ€‘tuning on the train split barely helps; overfitting the test split can â€œmemorizeâ€ small setups, underscoring real capacity limits in practical embedders .\n",
      "  - Reveals weak correlation between LIMIT and popular benchmarks (e.g., BEIR/MTEB), cautioning against overreliance on a single benchmark family and highlighting overfitting risks .\n",
      "- Tradeâ€‘offs:\n",
      "  - Stress tests surface worstâ€‘case behavior; they are invaluable for understanding limits but are intentionally hard by design .\n",
      "\n",
      "3) Retriever architecture choices\n",
      "- Singleâ€‘vector dense embeddings (standard â€œdense retrievalâ€)\n",
      "  - What it is: One vector per sequence; fast ANN search; widely used for generalization and instructionâ€‘following retrieval .\n",
      "  - Benefits: Scalable firstâ€‘stage retrieval; strong generalization across domains and tasks when queries are not highly combinatorial .\n",
      "  - Limits: Fundamentally dimensionâ€‘bounded; as tasks require connecting many otherwise unrelated documents (e.g., via logical operators), dense embedders miss combinations no matter the query; bigger d helps but cannot cover â€œallâ€ combinations at realistic scales  .\n",
      "- Multiâ€‘vector lateâ€‘interaction models (e.g., ColBERTâ€‘style)\n",
      "  - What it is: Multiple vectors per sequence with MaxSim to increase expressiveness .\n",
      "  - Benefits: Significantly outperforms singleâ€‘vector models on LIMIT, indicating higher combinational capacity .\n",
      "  - Tradeâ€‘offs: More compute/memory than singleâ€‘vector; less explored for instructionâ€‘following or reasoningâ€‘heavy retrieval tasks .\n",
      "- Crossâ€‘encoders / LLM rerankers\n",
      "  - What it is: Score queryâ€“document pairs jointly (often with longâ€‘context LLMs) as a reranking stage .\n",
      "  - Benefits: Extremely accurate; a longâ€‘context LLM reranker (Gemini 2.5 Pro) solved 100% of LIMITâ€‘small in one pass, bypassing singleâ€‘vector dimensional limits .\n",
      "  - Tradeâ€‘offs: Too expensive for firstâ€‘stage retrieval at scale; best used after an efficient retriever narrows candidates .\n",
      "- Sparse lexical/neural retrieval (e.g., BM25)\n",
      "  - What it is: Very highâ€‘dimensional sparse vectors keyed to terms/features .\n",
      "  - Benefits: Avoids dense bottlenecks; nearâ€‘perfect on LIMIT while dense models fail, thanks to effectively huge dimensionality .\n",
      "  - Tradeâ€‘offs: Weaker for instructionâ€‘following or reasoning definitions of â€œrelevanceâ€ where lexical overlap is minimal; applying sparse methods to such tasks remains open .\n",
      "\n",
      "4) Training and optimization methods for retrievers\n",
      "- What they are: Contrastive learning with MultipleNegativesRankingLoss and large inâ€‘batch negatives; dimensionality control via Matryoshka (MRL) and truncation experiments to study d â†’ performance; instructionâ€‘following finetuning (e.g., Promptriever)  .\n",
      "- Benefits:\n",
      "  - Clear evidence that higher embedding dimensionality boosts recall on combinationâ€‘heavy tasks; instructionâ€‘diverse training may better utilize the available embedding space .\n",
      "- Tradeâ€‘offs:\n",
      "  - Even with careful finetuning, singleâ€‘vector models remain dimensionâ€‘limited; training on heldâ€‘out data doesnâ€™t â€œfixâ€ the fundamental capacity constraint, while training on test induces overfitting rather than true general solution capacity .\n",
      "\n",
      "5) Benchmarking and evaluation methodology\n",
      "- What it is: Comparison of traditional benchmarks (BEIR/MTEB) to LIMIT; analysis of qrel graph density (how queries connect documents) as a key hardness factor; caution against benchmark overfitting and misgeneralization  .\n",
      "- Benefits:\n",
      "  - Encourages designing evaluations that probe combinational generalization, not just topical similarity, and that align with instructionâ€‘following/reasoning use cases .\n",
      "- Tradeâ€‘offs:\n",
      "  - Harder benchmarks can depress headline scores, but they reveal genuine system limits critical for robust deployments .\n",
      "\n",
      "Bottom line comparisons\n",
      "- If you need scalable firstâ€‘stage retrieval: singleâ€‘vector dense is efficient, but know its hard limit is embedding dimension; increase d and/or pair with a stronger second stage .\n",
      "- If you need higher recall on combinatorial or instructionâ€‘defined relevance: switch to multiâ€‘vector or add a crossâ€‘encoder/LLM reranker; they empirically overcome dense limits at higher compute cost  .\n",
      "- If your task aligns with lexical cues: sparse methods like BM25 can excel and avoid dense bottlenecks, though they may struggle when â€œrelevanceâ€ requires reasoning beyond overlap  .\n",
      "- For research methodology: combine theory (to know whatâ€™s impossible), targeted stress tests like LIMIT (to verify and quantify), and diversified benchmarks (to avoid overfitting blind spots)   .\n",
      "\n",
      "Finally, the documents argue these insights apply broadly across modalities for singleâ€‘vector embeddings; as tasks and instructions broaden, the same dimensionâ€‘driven limits surface, motivating more expressive architectures or multiâ€‘stage pipelines for robust AI systems  .\n",
      "\n",
      "ðŸ“š Documents Referenced:\n",
      "==============================\n",
      "ðŸ“„ Embedding-Based.pdf\n"
     ]
    }
   ],
   "source": [
    "# Ask a complex question that might require multiple documents\n",
    "complex_response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Compare and contrast different approaches to AI research and development mentioned in the documents. What are the key methodologies and their benefits?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id]\n",
    "    }],\n",
    "    include=[\"file_search_call.results\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸ”¬ Complex Multi-Document Query:\")\n",
    "print(\"=\" * 60)\n",
    "print(complex_response.output_text)\n",
    "\n",
    "# Show which documents were referenced\n",
    "print(\"\\nðŸ“š Documents Referenced:\")\n",
    "print(\"=\" * 30)\n",
    "for output_item in complex_response.output:\n",
    "    if output_item.type == \"message\":\n",
    "        for content_item in output_item.content:\n",
    "            if hasattr(content_item, 'annotations') and content_item.annotations:\n",
    "                unique_files = set()\n",
    "                for annotation in content_item.annotations:\n",
    "                    if annotation.type == \"file_citation\":\n",
    "                        unique_files.add(annotation.filename)\n",
    "                \n",
    "                for filename in unique_files:\n",
    "                    print(f\"ðŸ“„ {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016fc7fb",
   "metadata": {},
   "source": [
    "### Supported File Formats\n",
    "\n",
    "The file search tool supports a wide variety of file formats. Here's a summary of what you can upload:\n",
    "\n",
    "**Text and Code Files:**\n",
    "- `.py`, `.js`, `.ts`, `.java`, `.cpp`, `.c`, `.cs`, `.go`, `.php`, `.rb`, `.sh`\n",
    "- `.html`, `.css`, `.json`, `.md`, `.txt`, `.tex`\n",
    "\n",
    "**Document Formats:**\n",
    "- `.pdf` - PDF documents\n",
    "- `.doc`, `.docx` - Microsoft Word documents  \n",
    "- `.pptx` - PowerPoint presentations\n",
    "\n",
    "**Requirements:**\n",
    "- For text files, encoding must be UTF-8, UTF-16, or ASCII\n",
    "- Files are processed automatically for semantic search\n",
    "- Maximum file size and token limits apply (check OpenAI documentation for current limits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1513aa",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The OpenAI Responses API with File Search represents a powerful leap forward in building AI applications that can reason over your data. Key takeaways:\n",
    "\n",
    "âœ… **Hosted solution** - No need to manage your own vector database or search infrastructure  \n",
    "âœ… **Semantic search** - Find information by meaning, not just keywords  \n",
    "âœ… **Automatic citations** - Get references to source documents in responses  \n",
    "âœ… **Multiple file formats** - Support for PDFs, docs, code files, and more  \n",
    "âœ… **Retrieval customization** - Control search results and performance  \n",
    "âœ… **Easy integration** - Simple API that works seamlessly with the Responses API  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69175ab8",
   "metadata": {},
   "source": [
    "# MCP (Model Context Protocol) and Connectors\n",
    "\n",
    "In addition to file search, the OpenAI Responses API supports **MCP (Model Context Protocol)** servers and **Connectors** that give models the ability to connect to and control external services. This section demonstrates:\n",
    "\n",
    "## Two Types of External Integrations:\n",
    "\n",
    "### 1. **Connectors** \n",
    "- OpenAI-maintained MCP wrappers for popular services\n",
    "- Pre-built integrations like Google Workspace, Dropbox, Microsoft 365\n",
    "- OAuth-based authentication\n",
    "- Hosted by OpenAI with guaranteed reliability\n",
    "\n",
    "### 2. **Remote MCP Servers**\n",
    "- Third-party servers implementing the MCP protocol\n",
    "- Can be any server on the public Internet\n",
    "- Custom tools and capabilities\n",
    "- Examples: GitHub, Stripe, custom business tools\n",
    "\n",
    "## Key Benefits:\n",
    "- **Automatic tool discovery** - Models learn available tools dynamically\n",
    "- **Approval workflows** - Control what data is shared with external services  \n",
    "- **Real-time capabilities** - Access live data and perform actions\n",
    "- **Extensible** - Connect to virtually any service with an MCP server\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cf6fb",
   "metadata": {},
   "source": [
    "âš ï¸  **Please obtain a Google OAuth token first!**\n",
    "\n",
    "ðŸ“ **Steps to get a token:**\n",
    "1. Go to [Google OAuth Playground](https://developers.google.com/oauthplayground/)\n",
    "2. Enter scope: `https://www.googleapis.com/auth/calendar.events`\n",
    "3. Authorize APIs and exchange for token\n",
    "4. Replace the token in the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eab0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_OAUTH_TOKEN\"] = getpass.getpass(\"Enter your Google OAuth token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cad697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“… Google Calendar Response:\n",
      "==================================================\n",
      "I can pull that from your Google Calendar, but I need to confirm the year and time zone.\n",
      "\n",
      "- Friday, September 12 most recently occurred in 2025. Is that the date you mean, or a different year?\n",
      "- What time zone should I use for the dayâ€™s boundaries (e.g., your calendarâ€™s default)?\n",
      "\n",
      "Note: I tried to fetch your events just now but ran into a temporary connection error. If you confirm the year and time zone, Iâ€™ll try again immediately and return each eventâ€™s time, title, attendees, location/meet link, and notes.\n"
     ]
    }
   ],
   "source": [
    "# Google Calendar Connector Example\n",
    "# Note: You'll need to obtain an OAuth access token from Google\n",
    "# For testing, use Google's OAuth 2.0 Playground: https://developers.google.com/oauthplayground/\n",
    "\n",
    "# Example OAuth scope for Google Calendar:\n",
    "# https://www.googleapis.com/auth/calendar.events\n",
    "\n",
    "def demonstrate_google_calendar_connector(token):\n",
    "    \"\"\"\n",
    "    Demonstrate Google Calendar Connector functionality.\n",
    "    This example shows how to query calendar events using the Responses API.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Query today's calendar events\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            tools=[{\n",
    "                \"type\": \"mcp\",\n",
    "                \"server_label\": \"google_calendar\",\n",
    "                \"connector_id\": \"connector_googlecalendar\",\n",
    "                \"authorization\": token,\n",
    "                \"require_approval\": \"never\"  # Set to \"always\" for production\n",
    "            }],\n",
    "            input=\"What events do I have on my Google Calendar for Friday, September 12th? Please provide details about each event including time, title, and any important information.\"\n",
    "        )\n",
    "        \n",
    "        print(\"ðŸ“… Google Calendar Response:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(response.output_text)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with Google Calendar Connector: {e}\")\n",
    "        return None\n",
    "\n",
    "# Demonstrate the connector (will show setup instructions if no token provided)\n",
    "calendar_response = demonstrate_google_calendar_connector(os.environ[\"GOOGLE_OAUTH_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a04f4",
   "metadata": {},
   "source": [
    "#### Advanced Google Calendar Operations\n",
    "\n",
    "The Google Calendar Connector supports several sophisticated operations that can be combined for powerful calendar-based workflows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c871e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Advanced Calendar Analysis:\n",
      "==================================================\n",
      "Iâ€™m ready to do this, but I couldnâ€™t pull your events due to a temporary Google Calendar connection error. I attempted to fetch your events for this week (Oct 27â€“Nov 2, 2025) and received a 500 error from the calendar service. If youâ€™d like, I can retry immediately.\n",
      "\n",
      "Before I retry, please confirm a few details so I can be precise:\n",
      "- Timezone: Is America/Los_Angeles correct? If not, what timezone should I use?\n",
      "- Week definition: Do you want â€œthis weekâ€ as Monâ€“Sun or Sunâ€“Sat?\n",
      "- Calendars to include: Just your primary calendar, or also any others (e.g., work, shared)? If additional, please name them or share their IDs.\n",
      "- Working hours: What hours should I treat as your preferred working window (e.g., 9amâ€“5pm)?\n",
      "\n",
      "Alternatives if the connector continues to fail:\n",
      "- Share an .ics export for this week, or paste a list of your events (title, date, start/end times, location/meeting link, attendees, and whether theyâ€™re optional).\n",
      "- A screenshot of your weekly view also works.\n",
      "\n",
      "What I will deliver once I can access your events:\n",
      "1) Busiest days summary: Total meeting hours per day, meeting count, and the heaviest time blocks.\n",
      "2) Scheduling conflicts: Any overlaps, tight turnarounds, double-bookings, or travel/location risks.\n",
      "3) Optimal meeting times: Specific open windows in your working hours that minimize fragmentation and avoid existing holds, plus suggestions for buffer times and focus blocks.\n",
      "4) Total hours of scheduled meetings: For the week, excluding all-day/free/transparent events unless you want them included.\n",
      "\n",
      "Reply with the confirmations above and Iâ€™ll run the analysis right away.\n",
      "\n",
      "ðŸ” MCP Operations Performed:\n",
      "==============================\n",
      "\n",
      "ðŸ“‹ MCP Call 3:\n",
      "   Tool: search_events\n",
      "   Status: âŒ Error\n",
      "   Error: {'type': 'http_error', 'code': 500, 'message': 'An unknown error occurred while executing the tool.'}\n",
      "\n",
      "ðŸ“‹ MCP Call 5:\n",
      "   Tool: search_events\n",
      "   Status: âŒ Error\n",
      "   Error: {'type': 'http_error', 'code': 500, 'message': 'An unknown error occurred while executing the tool.'}\n",
      "\n",
      "ðŸ“‹ MCP Call 7:\n",
      "   Tool: search_events\n",
      "   Status: âŒ Error\n",
      "   Error: {'type': 'http_error', 'code': 500, 'message': 'An unknown error occurred while executing the tool.'}\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_advanced_calendar_operations(token):\n",
    "    \"\"\"\n",
    "    Demonstrate advanced Google Calendar operations including:\n",
    "    - Weekly schedule analysis\n",
    "    - Meeting conflict detection\n",
    "    - Event filtering and search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Advanced calendar analysis query\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            tools=[{\n",
    "                \"type\": \"mcp\",\n",
    "                \"server_label\": \"google_calendar\",\n",
    "                \"connector_id\": \"connector_googlecalendar\", \n",
    "                \"authorization\": token,\n",
    "                \"require_approval\": \"never\",\n",
    "                # Limit to specific tools for focused functionality\n",
    "                \"allowed_tools\": [\"search_events\", \"read_event\"]\n",
    "            }],\n",
    "            input=\"\"\"Analyze my Google Calendar for this week and provide:\n",
    "            1. A summary of my busiest days\n",
    "            2. Any potential scheduling conflicts\n",
    "            3. Recommendations for optimal meeting times\n",
    "            4. Total hours of scheduled meetings\n",
    "            \n",
    "            Please be thorough in your analysis and provide actionable insights.\"\"\"\n",
    "        )\n",
    "        \n",
    "        print(\"ðŸ“Š Advanced Calendar Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(response.output_text)\n",
    "        \n",
    "        # Examine the MCP calls that were made\n",
    "        print(\"\\nðŸ” MCP Operations Performed:\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        for i, output_item in enumerate(response.output):\n",
    "            if output_item.type == \"mcp_call\":\n",
    "                print(f\"\\nðŸ“‹ MCP Call {i + 1}:\")\n",
    "                print(f\"   Tool: {output_item.name}\")\n",
    "                print(f\"   Status: {'âœ… Success' if not output_item.error else 'âŒ Error'}\")\n",
    "                if output_item.error:\n",
    "                    print(f\"   Error: {output_item.error}\")\n",
    "                else:\n",
    "                    # Show a preview of the output\n",
    "                    output_preview = str(output_item.output)[:200]\n",
    "                    print(f\"   Output preview: {output_preview}...\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with advanced calendar operations: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run advanced calendar analysis\n",
    "advanced_calendar_response = demonstrate_advanced_calendar_operations(os.environ[\"GOOGLE_OAUTH_TOKEN\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddc496",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GitMCP Server\n",
    "\n",
    "The GitMCP server provides access to documentation and code repositories through a specialized MCP interface. Unlike connectors, this is a remote MCP server that implements the Model Context Protocol for accessing various code documentation resources.\n",
    "\n",
    "**Key Features:**\n",
    "- Documentation search and retrieval\n",
    "- Code analysis and explanation\n",
    "- Technical reference access\n",
    "- Real-time documentation fetching\n",
    "- Specialized tiktoken documentation access\n",
    "\n",
    "**Server Details:**\n",
    "- **Server URL:** `https://gitmcp.io/openai/tiktoken`\n",
    "- **Authentication:** None required for public documentation\n",
    "- **Protocol:** HTTP transport\n",
    "- **Maintained by:** GitMCP (third-party)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e0f65",
   "metadata": {},
   "source": [
    "â„¹ï¸ **No Authentication Required!**\n",
    "\n",
    "The GitMCP server for tiktoken documentation is publicly accessible and doesn't require any authentication tokens. This makes it perfect for demonstrating MCP functionality without setup complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0edd26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GitMCP server ready - no authentication required!\n"
     ]
    }
   ],
   "source": [
    "# No token required for GitMCP tiktoken documentation server\n",
    "print(\"âœ… GitMCP server ready - no authentication required!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65d02eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™ GitHub Analysis Response:\n",
      "==================================================\n",
      "tiktoken is OpenAIâ€™s fast tokenizer. It turns text into the integer â€œtokensâ€ that models consume and back again. Under the hood it uses a byte-level BPE (Byte Pair Encoding) scheme thatâ€™s trained for each family of models.\n",
      "\n",
      "How it works (conceptually)\n",
      "- Pre-tokenize: A regex (â€œpat_strâ€) splits UTF-8 text into chunks that roughly align with words, numbers, whitespace, and punctuation. This speeds up the next step.\n",
      "- Byte-level BPE merges: Each chunk is treated as a sequence of bytes. Using a learned â€œmerge ranksâ€ table, the algorithm repeatedly merges the most-preferred adjacent byte pairs until no higher-ranked merge applies. The final merged byte sequences map to integer token IDs.\n",
      "- Reversibility: Decoding is the inverseâ€”token IDs map back to byte sequences and then to the original text. If a sequence wasnâ€™t seen during training, the tokenizer can fall back to smaller units (down to raw bytes), so it always works on arbitrary text.\n",
      "- Special tokens: Encodings define reserved tokens (like <|endoftext|>, or chat/prompt markers). You can allow or disallow them during encoding.\n",
      "\n",
      "Why multiple â€œencodingsâ€\n",
      "- Each model family uses a specific vocabulary and merge-ranks file optimized for its training data:\n",
      "  - o200k_base: GPT-4o/omni family\n",
      "  - cl100k_base: GPT-4 / GPT-3.5-turbo family\n",
      "  - p50k_base, r50k_base, p50k_edit: older GPT-3 and code/edit use cases\n",
      "- tiktoken.encoding_for_model(model_name) picks the right one automatically.\n",
      "\n",
      "API at a glance\n",
      "- get_encoding(name): load a specific encoding by name.\n",
      "- encoding_for_model(model): load the recommended encoding for a model.\n",
      "- encode(text, allowed_special=..., disallowed_special=...): text â†’ list[int].\n",
      "- decode(tokens): list[int] â†’ text.\n",
      "- Batch variants exist for speed on many strings.\n",
      "\n",
      "Performance/implementation\n",
      "- Written in Rust with Python bindings, designed for speed (often 3â€“6Ã— faster than common alternatives on large corpora).\n",
      "- Ships with the precompiled merge ranks and patterns used by OpenAI models.\n",
      "\n",
      "Rules of thumb\n",
      "- In English, 1 token â‰ˆ 3â€“4 characters on average; this varies by language and content.\n",
      "- Counting tokens is essential for staying within model context limits; tiktoken gives you the exact count your model will use.\n",
      "\n",
      "Minimal example\n",
      "- Python:\n",
      "  import tiktoken\n",
      "  enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
      "  ids = enc.encode(\"Hello, world!\")\n",
      "  text = enc.decode(ids)\n",
      "\n",
      "If youâ€™re curious about the algorithm details or want to build/customize an encoding, the repo includes an educational module and a plugin mechanism to register new encodings.\n"
     ]
    }
   ],
   "source": [
    "# GitMCP Server Example\n",
    "\n",
    "def demonstrate_github_mcp_server():\n",
    "    \"\"\"\n",
    "    Demonstrate GitMCP server functionality.\n",
    "    Shows how to interact with GitHub repositories using the official GitMCP server.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        tools=[{\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_label\": \"gitmcp\",\n",
    "            \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n",
    "            \"allowed_tools\": [\"search_tiktoken_documentation\", \"fetch_tiktoken_documentation\"],\n",
    "            \"require_approval\": \"never\"  # Set to \"always\" for production\n",
    "        }],\n",
    "        input=\"\"\"How does TikToken work?\"\"\"\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ™ GitHub Analysis Response:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(response.output_text)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Demonstrate the GitMCP server\n",
    "github_response = demonstrate_github_mcp_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae386faf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Summary: OpenAI Responses API with Data and Connectors\n",
    "\n",
    "This notebook demonstrated the full capabilities of the OpenAI Responses API for building AI applications that can access and reason over external data sources. Here's what we covered:\n",
    "\n",
    "### ðŸ” File Search Capabilities\n",
    "- **Vector Store Management** - Creating and managing knowledge bases\n",
    "- **Multi-format Support** - PDFs, documents, code files, and more  \n",
    "- **Semantic Search** - Finding information by meaning, not just keywords\n",
    "- **Automatic Citations** - Getting references to source documents\n",
    "- **Retrieval Customization** - Controlling search results and performance\n",
    "\n",
    "### ðŸ”Œ MCP Servers and Connectors\n",
    "- **Google Calendar Connector** - Reading and analyzing calendar events with OAuth\n",
    "- **GitMCP** - Checking out the GitHub Docs!\n",
    "\n",
    "### ðŸ›¡ï¸ Security and Best Practices\n",
    "- **OAuth Authentication** - Secure token management for external services\n",
    "- **Approval Controls** - Fine-grained permission management\n",
    "- **Error Recovery** - Retry logic and graceful failure handling\n",
    "- **Data Privacy** - Understanding what data is shared with external services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activate (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
