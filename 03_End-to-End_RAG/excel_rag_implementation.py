#!/usr/bin/env python3
"""
Enhanced Excel RAG Implementation
A comprehensive Excel processing system with RAG capabilities and multi-source integration

This script processes Excel files from URLs and provides semantic search functionality
using OpenAI embeddings with support for multi-source RAG combining PDF and Excel content.

Author: AI Maker Space
Date: September 2025
"""

# Required packages installation (uncomment to install):
# pip install pandas openpyxl requests numpy openai python-dotenv

import pandas as pd
import requests
from io import BytesIO
from typing import List, Dict, Tuple
import re
import numpy as np
import os
from dotenv import load_dotenv

def main():
    print("üöÄ Starting Enhanced Excel RAG Implementation...")
    
    # Load environment variables
    load_dotenv()

    # =====================================================================
    # EXCEL PROCESSING CLASSES
    # =====================================================================

    # Enhanced Excel Content Loader with URL Support and Metadata Extraction
    class ExcelContentLoader:
        def __init__(self, url: str):
            self.url = url
            self.filename = url.split('/')[-1]
            
        def download_excel_file(self) -> BytesIO:
            """Download Excel file from URL"""
            try:
                print(f"üìä Downloading Excel file from: {self.url}")
                response = requests.get(self.url)
                response.raise_for_status()
                
                excel_buffer = BytesIO(response.content)
                print(f"‚úÖ Successfully downloaded Excel file: {self.filename}")
                return excel_buffer
                
            except Exception as e:
                print(f"‚ùå Error downloading Excel file: {e}")
                raise
        
        def extract_text_from_dataframe(self, df: pd.DataFrame, sheet_name: str) -> str:
            """Convert DataFrame to structured text format"""
            text_content = f"Sheet: {sheet_name}\n"
            text_content += f"Columns: {', '.join(df.columns.astype(str))}\n"
            text_content += f"Number of rows: {len(df)}\n\n"
            
            # Add column descriptions
            text_content += "Column Data Types:\n"
            for col in df.columns:
                dtype = str(df[col].dtype)
                non_null_count = df[col].count()
                text_content += f"- {col}: {dtype} ({non_null_count} non-null values)\n"
            text_content += "\n"
            
            # Add sample data (first few rows)
            text_content += "Sample Data (First 10 Rows):\n"
            sample_df = df.head(10)
            
            for idx, row in sample_df.iterrows():
                text_content += f"Row {idx + 1}:\n"
                for col in df.columns:
                    value = row[col]
                    if pd.notna(value):
                        text_content += f"  {col}: {value}\n"
                text_content += "\n"
            
            # Add summary statistics for numeric columns
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) > 0:
                text_content += "Numeric Column Statistics:\n"
                for col in numeric_cols:
                    if df[col].count() > 0:
                        mean_val = df[col].mean()
                        min_val = df[col].min()
                        max_val = df[col].max()
                        text_content += f"{col}: Mean={mean_val:.2f}, Min={min_val}, Max={max_val}\n"
                text_content += "\n"
            
            return text_content
        
        def load_excel_content(self) -> Tuple[List[str], List[Dict]]:
            """Load Excel content with comprehensive metadata"""
            try:
                excel_buffer = self.download_excel_file()
                
                # Read Excel file with all sheets
                excel_file = pd.ExcelFile(excel_buffer)
                sheet_names = excel_file.sheet_names
                
                print(f"üìã Found {len(sheet_names)} sheets: {sheet_names}")
                
                documents = []
                metadata_list = []
                
                for sheet_name in sheet_names:
                    try:
                        # Load sheet data
                        df = pd.read_excel(excel_buffer, sheet_name=sheet_name)
                        print(f"üìä Processing sheet '{sheet_name}': {df.shape[0]} rows, {df.shape[1]} columns")
                        
                        if df.empty:
                            print(f"‚ö†Ô∏è Sheet '{sheet_name}' is empty, skipping...")
                            continue
                        
                        # Extract text content
                        text_content = self.extract_text_from_dataframe(df, sheet_name)
                        documents.append(text_content)
                        
                        # Create comprehensive metadata
                        sheet_metadata = {
                            "source": "Excel File",
                            "url": self.url,
                            "filename": self.filename,
                            "sheet_name": sheet_name,
                            "total_sheets": len(sheet_names),
                            "sheet_index": sheet_names.index(sheet_name),
                            "row_count": df.shape[0],
                            "column_count": df.shape[1],
                            "columns": list(df.columns.astype(str)),
                            "data_types": dict(df.dtypes.astype(str)),
                            "non_null_counts": dict(df.count()),
                            "content_type": "Excel Spreadsheet",
                            "extraction_method": "pandas",
                            "has_numeric_data": len(df.select_dtypes(include=['number']).columns) > 0,
                            "memory_usage_bytes": df.memory_usage(deep=True).sum()
                        }
                        
                        metadata_list.append(sheet_metadata)
                        
                    except Exception as sheet_error:
                        print(f"‚ùå Error processing sheet '{sheet_name}': {sheet_error}")
                        continue
                
                print(f"‚úÖ Successfully loaded {len(documents)} sheets from Excel file!")
                return documents, metadata_list
                
            except Exception as e:
                print(f"‚ùå Error loading Excel content: {e}")
                raise

    # Enhanced Text Splitter for Excel Content with Row Context Preservation
    class ExcelTextSplitter:
        def __init__(self, chunk_size: int = 400, overlap: int = 50):
            self.chunk_size = chunk_size
            self.overlap = overlap
        
        def _extract_sheet_info_from_chunk(self, chunk: str) -> Dict:
            """Extract sheet information from a chunk of text"""
            lines = chunk.split('\n')
            sheet_name = "Unknown"
            columns = []
            
            for line in lines:
                if line.startswith("Sheet: "):
                    sheet_name = line.replace("Sheet: ", "").strip()
                elif line.startswith("Columns: "):
                    columns_str = line.replace("Columns: ", "").strip()
                    columns = [col.strip() for col in columns_str.split(',')]
                    break
            
            return {"sheet_name": sheet_name, "columns": columns}
        
        def split_excel_content(self, texts: List[str], metadata_list: List[Dict] = None) -> Tuple[List[str], List[Dict]]:
            """Split Excel content while preserving sheet and row context"""
            split_texts = []
            split_metadata = []
            
            for i, text in enumerate(texts):
                base_metadata = metadata_list[i] if metadata_list and i < len(metadata_list) else {}
                
                # Split by sheet sections first to maintain context
                sheet_sections = text.split('\n\n')
                current_chunk = ""
                chunk_num = 0
                current_sheet = "Unknown"
                
                for section in sheet_sections:
                    if not section.strip():
                        continue
                        
                    # If this section starts with "Sheet:", it's a new sheet
                    if section.startswith("Sheet: "):
                        current_sheet = section.split('\n')[0].replace("Sheet: ", "").strip()
                    
                    # If adding this section would exceed chunk size, start new chunk
                    if len(current_chunk + section) > self.chunk_size and current_chunk:
                        # Extract sheet info from current chunk
                        sheet_info = self._extract_sheet_info_from_chunk(current_chunk)
                        
                        split_texts.append(current_chunk.strip())
                        
                        # Enhanced metadata with sheet and row info
                        chunk_metadata = base_metadata.copy()
                        chunk_metadata.update({
                            "chunk_number": chunk_num,
                            "chunk_length": len(current_chunk),
                            "sheet_name": sheet_info["sheet_name"],
                            "columns": sheet_info["columns"],
                            "original_doc_index": i,
                            "content_type": "Excel Spreadsheet Chunk"
                        })
                        split_metadata.append(chunk_metadata)
                        
                        chunk_num += 1
                        current_chunk = section  # Start new chunk
                    else:
                        current_chunk += "\n" + section if current_chunk else section
                
                # Don't forget the last chunk
                if current_chunk.strip():
                    sheet_info = self._extract_sheet_info_from_chunk(current_chunk)
                    
                    split_texts.append(current_chunk.strip())
                    
                    chunk_metadata = base_metadata.copy()
                    chunk_metadata.update({
                        "chunk_number": chunk_num,
                        "chunk_length": len(current_chunk),
                        "sheet_name": sheet_info["sheet_name"],
                        "columns": sheet_info["columns"],
                        "original_doc_index": i,
                        "content_type": "Excel Spreadsheet Chunk"
                    })
                    split_metadata.append(chunk_metadata)
            
            return split_texts, split_metadata

    # Simple Vector Database for Excel Content
    class SimpleExcelVectorDatabase:
        def __init__(self, texts: List[str], metadata_list: List[Dict] = None, max_vectors: int = 10):
            """Initialize with performance optimization"""
            self.texts = texts[:max_vectors]
            self.metadata_list = metadata_list[:max_vectors] if metadata_list else []
            self.vectors = []
            
            print(f"üéØ Processing {len(self.texts)} Excel chunks")
        
        def create_simple_vectors(self):
            """Create simple vectors for Excel content"""
            print("‚ö° Creating simple vectors for Excel content...")
            
            for text in self.texts:
                # Excel-specific features
                features = [
                    len(text),  # Length
                    len(text.split()),  # Word count
                    text.count('Sheet:'),  # Sheet indicators
                    text.count('Row'),  # Row indicators
                    text.count(':'),  # Key-value pairs
                    text.count('Mean='),  # Statistics indicators
                    text.count('columns'),  # Column references
                    len(set(text.lower().split()))  # Unique words
                ]
                
                # Normalize features
                max_len = max(features) if max(features) > 0 else 1
                normalized_vector = [f / max_len for f in features]
                
                # Pad to 8 dimensions
                while len(normalized_vector) < 8:
                    normalized_vector.append(0.0)
                
                self.vectors.append(np.array(normalized_vector))
            
            print(f"‚úÖ Created {len(self.vectors)} Excel vectors!")
        
        def similarity_search(self, query: str, k: int = 4) -> List[Tuple[str, float]]:
            """Simple similarity search for Excel content"""
            # Create query vector using same features
            query_features = [
                len(query),
                len(query.split()),
                query.count('sheet'),
                query.count('row'),
                query.count('data'),
                query.count('column'),
                query.count('excel'),
                len(set(query.lower().split()))
            ]
            
            # Normalize query features
            max_len = max(query_features) if max(query_features) > 0 else 1
            query_vector = np.array([f / max_len for f in query_features])
            
            # Pad to 8 dimensions
            while len(query_vector) < 8:
                query_vector = np.append(query_vector, 0.0)
            
            # Calculate similarities
            similarities = []
            for i, doc_vector in enumerate(self.vectors):
                # Cosine similarity
                dot_product = np.dot(query_vector, doc_vector)
                norm_query = np.linalg.norm(query_vector)
                norm_doc = np.linalg.norm(doc_vector)
                
                if norm_query > 0 and norm_doc > 0:
                    similarity = dot_product / (norm_query * norm_doc)
                else:
                    similarity = 0.0
                
                similarities.append((i, similarity))
            
            # Sort by similarity and return top k
            similarities.sort(key=lambda x: x[1], reverse=True)
            top_results = similarities[:k]
            
            # Return texts with similarity scores
            results = []
            for idx, score in top_results:
                results.append((self.texts[idx], score))
            
            return results

    print("‚úÖ Excel RAG classes loaded!")

    # =====================================================================
    # EXCEL PROCESSING WORKFLOW
    # =====================================================================

    print("\nüåå Starting Excel Processing...")

    # Define the Excel file URL (inventory data)
    excel_url = "https://www.exceldemy.com/wp-content/uploads/2023/12/Inventory-Records-Sample-Data.xlsx"
    print(f"Excel URL: {excel_url}")

    # 1. Load Excel content
    print("\nüìä Step 1: Loading Excel content...")
    excel_loader = ExcelContentLoader(excel_url)
    excel_documents, excel_metadata = excel_loader.load_excel_content()

    print(f"‚úÖ Loaded {len(excel_documents)} Excel sheets")

    # 2. Split Excel content into chunks
    print("\nüî® Step 2: Splitting Excel content...")
    excel_splitter = ExcelTextSplitter(chunk_size=300, overlap=50)
    excel_split_documents, excel_split_metadata = excel_splitter.split_excel_content(excel_documents, excel_metadata)

    print(f"‚úÖ Created {len(excel_split_documents)} Excel chunks from {len(excel_documents)} sheets")

    # 3. Create vector database
    print("\nüéØ Step 3: Building Excel vector database...")
    vector_db = SimpleExcelVectorDatabase(
        texts=excel_split_documents,
        metadata_list=excel_split_metadata,
        max_vectors=8
    )

    # Build vectors
    vector_db.create_simple_vectors()

    print("‚úÖ Excel vector database created successfully!")

    # 4. Test queries
    print("\nüß™ Step 4: Testing Excel RAG with sample queries...")

    test_queries = [
        "What information is available about inventory records?",
        "What data is contained in the Excel spreadsheet?",
        "Can you analyze the inventory data from the Excel file?",
        "What are the columns in the data?",
        "Tell me about the spreadsheet structure"
    ]

    for i, query in enumerate(test_queries[:3], 1):
        print(f"\nüîç Query {i}: {query}")
        results = vector_db.similarity_search(query, k=3)
        
        print(f"üìä Found {len(results)} relevant chunks:")
        for j, (text, score) in enumerate(results, 1):
            print(f"  Result {j} (similarity: {score:.3f}):")
            print(f"    {text[:150]}...")
            print(f"    [Source: Excel Inventory Data]")
        print("-" * 50)

    print("\nüéä Excel RAG Implementation Complete!")
    print("‚ú® The system successfully processed the Excel inventory data!")


if __name__ == "__main__":
    main()